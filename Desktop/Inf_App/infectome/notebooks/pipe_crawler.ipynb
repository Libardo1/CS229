{
 "metadata": {
  "name": "",
  "signature": "sha256:81911ed6924c6c32fdcce5f0870d36c7678ebf68606ec6e8ce43cb426fb27df3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt \n",
      "import pandas as pd\n",
      "import glob \n",
      "import scipy\n",
      "import sqlite3\n",
      "import datetime\n",
      "from pandas.io import sql\n",
      "import pickle\n",
      "%matplotlib inline\n",
      "matplotlib.rcParams['savefig.dpi'] = 3 * matplotlib.rcParams['savefig.dpi']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlalchemy\n",
      "from sqlalchemy import create_engine\n",
      "engine = create_engine('postgresql://postgres@localhost:5432/infectome')\n",
      "conn = engine.connect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Write once - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GI length table\n",
      "db_fi='NCBIGenomes06'\n",
      "giDict={}\n",
      "GI_file=\"/quakestor2/kertesz/soup/Data/GenomeDB/%s/tmp_seq.stab\"%db_fi\n",
      "with open(GI_file) as f:\n",
      "    for line in f:\n",
      "        parts=line.split('\\t')\n",
      "        gi_name=parts[0].split('|')\n",
      "        # Note - Value must be stored as list for later upload into pandas.\n",
      "        giDict[gi_name[0]+'|'+gi_name[1]]=[len(parts[1])]\n",
      "df_giDict=pd.DataFrame(giDict)\n",
      "df_giDict=df_giDict.transpose()\n",
      "df_giDict.columns=['gi_length']\n",
      "df_giDict.reset_index(inplace=True)\n",
      "df_giDict.to_sql('gi_length_table', engine, if_exists='replace')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path='/quakestor2/llmartin/apps/infectome/inputs/min.tax.table'\n",
      "FullTaxTable=pd.DataFrame(pd.read_table(path,dtype=str))\n",
      "FullTaxTable.set_index('tax_id',inplace=True,drop=False)\n",
      "\n",
      "taxEncoding=pd.DataFrame()\n",
      "taxEncoding.loc['ASPERGILLUS','taxID']='5061' # Vague: Use ASPERGILLUS NIGER.\n",
      "taxEncoding.loc['ASPERGILLUS NIGER','taxID']='5061'\n",
      "taxEncoding.loc['ASPERGILLUS TERREUS','taxID']='33178' \n",
      "taxEncoding.loc['ASPERGILLUS FUMIGATUS','taxID']='746128'\n",
      "taxEncoding.loc['Emericella nidulans','taxID']='162425' # Also known as Aspergillus nidulans\n",
      "taxEncoding.loc['C. Diff','taxID']='1496' \n",
      "taxEncoding.loc['CANDIDA SPECIES','taxID']='5476' # Vague: Use CANDIDA ALBICANS.\n",
      "taxEncoding.loc['CANDIDA ALBICANS','taxID']='5476' \n",
      "taxEncoding.loc['CANDIDA GLABRATA','taxID']='5478'\n",
      "taxEncoding.loc['CANDIDA PARAPSILOSIS','taxID']='5480'\n",
      "taxEncoding.loc['M.PSEUDO','taxID']='287' \n",
      "taxEncoding.loc['PSEUDOMONAS AERUGINOSA','taxID']='287' \n",
      "taxEncoding.loc['CUNNINGHAMELLA','taxID']='155726' # Vague: Use only species recorded.\n",
      "taxEncoding.loc['STREPTOCOCCUS','taxID']='1313' # Vague: Use Streptococcus pneumoniae species.\n",
      "taxEncoding.loc['STREPTOCOCCUS SALIVARIUS','taxID']='1304' \n",
      "taxEncoding.loc['STREPTOOCOCCUS ANGINOSUS','taxID']='1328' \n",
      "taxEncoding.loc['EBV','taxID']='10376' \n",
      "taxEncoding.loc['CMV','taxID']='10359'\n",
      "taxEncoding.loc['Mycobacterium chelonae','taxID']='670516'\n",
      "taxEncoding.loc['ALTERNARIA SPECIES','taxID']='445261' # Vague: Use Alternaria multiformis.\n",
      "taxEncoding.loc['MRSA','taxID']='1280'\n",
      "taxEncoding.loc['Staph Aureus','taxID']='1280'\n",
      "taxEncoding.loc['COAG NEGATIVE STAPHYLOCOCCUS','taxID']='1280'\n",
      "taxEncoding.loc['PENICILLIUM','taxID']='69763' # Vague: Use Penicillium adametzii.\n",
      "taxEncoding.loc['Enterobacter Aerogenes','taxID']='548'\n",
      "taxEncoding.loc['YEAST','taxID']='4932' # Vague: Use Saccharomyces cerevisiae\n",
      "taxEncoding.loc['SERRATIA RUBIDAEA','taxID']='614' # Not in tax table: Use S. liquefaciens\n",
      "taxEncoding.loc['CORYNEBACTERIUM STRIATUM','taxID']='43770'\n",
      "taxEncoding.loc['CORYNE. PSEUDODIPHTHERICTICUM','taxID']='1719' # Not in tax table: Use Corynebacterium pseudotuberculosis\n",
      "taxEncoding.loc['Mycobacterium gordonae','taxID']='1764' # Not in tax table: Use M. avium\n",
      "taxEncoding.loc['KLEBSIELLA PNEUMONIA','taxID']='573'\n",
      "taxEncoding.loc['BK Virus','taxID']='10629'\n",
      "taxEncoding.loc['MICROCOCCUS','taxID']='1270' # Vague: Use M. lutens\n",
      "taxEncoding.loc['MYCOBACTERIUM AVIUM COMPLEX','taxID']='1764'\n",
      "taxEncoding.loc['ADENOVIRUS','taxID']='108098' # Vague: Use human adenovirus B\n",
      "taxEncoding.loc['KLEBSIELLA OXYTOCA','taxID']='571'\n",
      "taxEncoding.loc['Cladosporium species','taxID']='29917' # Vague: Use Cladosporium cladosporioides\n",
      "taxEncoding.loc['E.COLI','taxID']='562'\n",
      "taxEncoding.loc['ACHROMOBACTER XYLOSOXIDANS','taxID']='85698'\n",
      "taxEncoding.loc['ENTEROCOCCUS','taxID']='1351' # Vague: Use Enterococcus faecalis\n",
      "taxEncoding.loc['Microsporidia','taxID']='31281' # Vague: Use Enterocytozoon bieneusi\n",
      "taxEncoding.loc['STENOTROPHOMONAS MALTOPHILIA' ,'taxID']='40324'\n",
      "taxEncoding.loc['Citrobacter Freundii','taxID']='546'\n",
      "taxEncoding.loc['CHRYSEOBACTERIUM INDOLOGENES','taxID']='531844' # Not in tax table: Use species under Flavobacterium\n",
      "taxEncoding.loc['HAEMOPHILUS INFLUENZA','taxID']='727'\n",
      "taxEncoding.loc['NOCARDIA CYRIACIGEORGICA','taxID']='135487'\n",
      "taxEncoding.loc['HHV6','taxID']='32603' \n",
      "\n",
      "def getName(taxID):\n",
      "    name=FullTaxTable.loc[taxID,'tax_name']\n",
      "    return name\n",
      "\n",
      "def getRank(taxID):\n",
      "    name=FullTaxTable.loc[taxID,'rank']\n",
      "    return name\n",
      "\n",
      "def getFamily(taxID):\n",
      "    fam=FullTaxTable.loc[taxID,'family']\n",
      "    return fam\n",
      "\n",
      "def getSpecies(taxID):\n",
      "    spe=FullTaxTable.loc[taxID,'species']\n",
      "    return spe  \n",
      "\n",
      "def getGenus(taxID):\n",
      "    gen=FullTaxTable.loc[taxID,'genus']\n",
      "    return gen\n",
      "\n",
      "def getOrder(taxID):\n",
      "    gen=FullTaxTable.loc[taxID,'order']\n",
      "    return gen\n",
      "\n",
      "def getClass(taxID):\n",
      "    gen=FullTaxTable.loc[taxID,'class']\n",
      "    return gen\n",
      "\n",
      "def getPhylum(taxID):\n",
      "    gen=FullTaxTable.loc[taxID,'phylum']\n",
      "    return gen\n",
      "\n",
      "def getKingdom(taxID):\n",
      "    gen=FullTaxTable.loc[taxID,'superkingdom']\n",
      "    return gen\n",
      "\n",
      "taxEncoding['Name']=taxEncoding['taxID'].apply(getName)\n",
      "taxEncoding['Rank']=taxEncoding['taxID'].apply(getRank)\n",
      "taxEncoding['species']=taxEncoding['taxID'].apply(getSpecies)\n",
      "taxEncoding['genus']=taxEncoding['taxID'].apply(getGenus)\n",
      "taxEncoding['family']=taxEncoding['taxID'].apply(getFamily)\n",
      "taxEncoding['order']=taxEncoding['taxID'].apply(getOrder)\n",
      "taxEncoding['class']=taxEncoding['taxID'].apply(getClass)\n",
      "taxEncoding['phylum']=taxEncoding['taxID'].apply(getPhylum)\n",
      "taxEncoding['kingdom']=taxEncoding['taxID'].apply(getKingdom)\n",
      "taxEncoding['Clinical_ID']=taxEncoding.index\n",
      "taxEncoding.columns = map(str.lower, taxEncoding.columns)\n",
      "taxEncoding.to_sql('clinical_tax_encoding_table', engine, if_exists='replace')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Script - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inputs\n",
      "samples=['I*','L*','PR*','bP*']\n",
      "samples=['bP*'] # sample_input='I6_W1'\n",
      "dbHandle='NCBIGenomes06'\n",
      "version='V2.2'\n",
      "fi_dir='/quakestor2/kertesz/soup/Runs/%s/Samples/'%version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Aggregate raw data stored in files\n",
      "def makeDF(files,add_id):\n",
      "    ''' Make dataframes to hold data for target files '''\n",
      "    DF = pd.DataFrame()\n",
      "    tmp = []\n",
      "    for fi in files:\n",
      "        try:\n",
      "            df=pd.DataFrame(pd.read_table(fi,header=None))\n",
      "            if add_id:\n",
      "                path,fname=os.path.split(fi)\n",
      "                sample_id=fname.split('.tblat.1')[0]\n",
      "                patient_id=sample_id.split('_')[0]\n",
      "                df['Sample_ID']=sample_id\n",
      "                df['Patient_ID']=patient_id\n",
      "            tmp.append(df)\n",
      "        except ValueError:\n",
      "            print \"Possible empty file for %s\"%fi\n",
      "    DF=pd.concat(tmp)\n",
      "    return DF\n",
      "\n",
      "stor_blast=[]\n",
      "stor_seq_stat=[]\n",
      "stor_gra_dat=[]\n",
      "\n",
      "for samhandle in samples:  \n",
      "    \n",
      "    # Blast files \n",
      "    blastfiles=glob.glob(fi_dir+samhandle+'/03-GRAMMy/'+dbHandle+'/*.tblat.1')  \n",
      "    blastfi=makeDF(blastfiles,1)\n",
      "    blastfi.columns=['Read','GI','%_Identity','Alignment_length','Mismatches','Gap_opens','qStart','qEnd','sStart','sEnd','eValue','bit Score','TaxID','Name','Kingdom','Sample_ID','Patient_ID']\n",
      "    blastfi['GI']=blastfi['GI'].apply(lambda x:x.split('|')[0]+'|'+x.split('|')[1])\n",
      "    blastfi['Patient_ID']=blastfi['Sample_ID'].apply(lambda x:x.split('_')[0])\n",
      "    blastfi=blastfi[['GI','sStart','sEnd','TaxID','Sample_ID','Name','Patient_ID']]\n",
      "    blastfi.drop_duplicates(inplace=True)\n",
      "    stor_blast=stor_blast+[blastfi]\n",
      "    \n",
      "    # Stats files\n",
      "    graStatFiles=glob.glob(fi_dir+samhandle+'/03-GRAMMy/'+dbHandle+'/*stats.tab')  \n",
      "    graStatDF=makeDF(graStatFiles,0)\n",
      "    mapStatFiles=glob.glob(fi_dir+samhandle+'/01-Align/*stats.tab')\n",
      "    mapStatDF=makeDF(mapStatFiles,0)\n",
      "    all_stats=pd.concat([graStatDF,mapStatDF])\n",
      "    all_stats.columns=['Pipeline','Sample_ID','Stage','Stat','Value']\n",
      "    all_stats['Database']=dbHandle\n",
      "    if projectHandle=='SCS*': # Sample_ID fix and Coverage fix (chr21) for SCS \n",
      "        all_stats.loc[all_stats['Stat']=='chr21_coverage','Value']=40  \n",
      "        all_stats['Sample_ID']=all_stats['Sample_ID'].apply(lambda x:x.split('_')[0])+all_stats['Sample_ID'].apply(lambda x:x.split('_')[1])+'_'+all_stats['Sample_ID'].apply(lambda x:x.split('_')[1])\n",
      "    stor_seq_stat=stor_seq_stat+[all_stats]\n",
      "    \n",
      "    # GRAMMyData table \n",
      "    graFiles = [fn for fn in glob.glob(fi_dir+samhandle+'/03-GRAMMy/'+dbHandle+'/*.tab') if 'stats' not in fn]\n",
      "    GRAMMyData=makeDF(graFiles,0)\n",
      "    GRAMMyData.columns=['Sample_ID','Tax_ID','Abundance','Error']\n",
      "    GRAMMyData['Tax_ID']=GRAMMyData['Tax_ID'].astype(str)\n",
      "    GRAMMyData['Pipeline']=np.unique(all_stats['Pipeline'])[0]\n",
      "    GRAMMyData['Database']=dbHandle\n",
      "    stor_gra_dat=stor_gra_dat+[GRAMMyData]\n",
      "    \n",
      "blastfi=pd.concat(stor_blast,axis=0)\n",
      "all_stats=pd.concat(stor_seq_stat,axis=0)\n",
      "GRAMMyData=pd.concat(stor_gra_dat,axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/I8_W5Y/03-GRAMMy/NCBIGenomes06/I8_W5Y.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/I8_W5Y/03-GRAMMy/NCBIGenomes06/I8_W5Y.tab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L77_W1/03-GRAMMy/NCBIGenomes06/L77_W1.tblat.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L8_W2/03-GRAMMy/NCBIGenomes06/L8_W2.tblat.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M6/03-GRAMMy/NCBIGenomes06/L21_M6.tblat.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M1-75/03-GRAMMy/NCBIGenomes06/L21_M1-75.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M1/03-GRAMMy/NCBIGenomes06/L21_M1.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_W3/03-GRAMMy/NCBIGenomes06/L21_W3.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M4/03-GRAMMy/NCBIGenomes06/L21_M4.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M3-5/03-GRAMMy/NCBIGenomes06/L21_M3-5.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L22_M35/03-GRAMMy/NCBIGenomes06/L22_M35.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L22_M30/03-GRAMMy/NCBIGenomes06/L22_M30.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M2/03-GRAMMy/NCBIGenomes06/L26_M2.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M4-5/03-GRAMMy/NCBIGenomes06/L26_M4-5.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M9/03-GRAMMy/NCBIGenomes06/L26_M9.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_W2/03-GRAMMy/NCBIGenomes06/L26_W2.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M3/03-GRAMMy/NCBIGenomes06/L26_M3.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M39/03-GRAMMy/NCBIGenomes06/L26_M39.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M8/03-GRAMMy/NCBIGenomes06/L26_M8.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M1-5/03-GRAMMy/NCBIGenomes06/L26_M1-5.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M6/03-GRAMMy/NCBIGenomes06/L26_M6.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L77_W1/03-GRAMMy/NCBIGenomes06/L77_W1.tab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L8_W2/03-GRAMMy/NCBIGenomes06/L8_W2.tab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M6/03-GRAMMy/NCBIGenomes06/L21_M6.tab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M1-75/03-GRAMMy/NCBIGenomes06/L21_M1-75.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M1/03-GRAMMy/NCBIGenomes06/L21_M1.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_W3/03-GRAMMy/NCBIGenomes06/L21_W3.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M4/03-GRAMMy/NCBIGenomes06/L21_M4.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L21_M3-5/03-GRAMMy/NCBIGenomes06/L21_M3-5.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L22_M35/03-GRAMMy/NCBIGenomes06/L22_M35.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L22_M30/03-GRAMMy/NCBIGenomes06/L22_M30.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M2/03-GRAMMy/NCBIGenomes06/L26_M2.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M4-5/03-GRAMMy/NCBIGenomes06/L26_M4-5.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M9/03-GRAMMy/NCBIGenomes06/L26_M9.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_W2/03-GRAMMy/NCBIGenomes06/L26_W2.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M3/03-GRAMMy/NCBIGenomes06/L26_M3.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M39/03-GRAMMy/NCBIGenomes06/L26_M39.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M8/03-GRAMMy/NCBIGenomes06/L26_M8.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M1-5/03-GRAMMy/NCBIGenomes06/L26_M1-5.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/L26_M6/03-GRAMMy/NCBIGenomes06/L26_M6.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_24/03-GRAMMy/NCBIGenomes06/PR10031_24.tblat.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_17/03-GRAMMy/NCBIGenomes06/PR10025_17.tblat.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_27/03-GRAMMy/NCBIGenomes06/PR10025_27.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_38/03-GRAMMy/NCBIGenomes06/PR10025_38.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_9/03-GRAMMy/NCBIGenomes06/PR10025_9.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10029_16/03-GRAMMy/NCBIGenomes06/PR10029_16.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10029_8/03-GRAMMy/NCBIGenomes06/PR10029_8.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_14/03-GRAMMy/NCBIGenomes06/PR10031_14.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_29/03-GRAMMy/NCBIGenomes06/PR10031_29.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_8/03-GRAMMy/NCBIGenomes06/PR10031_8.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_19/03-GRAMMy/NCBIGenomes06/PR10047_19.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_27/03-GRAMMy/NCBIGenomes06/PR10047_27.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_39/03-GRAMMy/NCBIGenomes06/PR10047_39.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_9/03-GRAMMy/NCBIGenomes06/PR10047_9.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR19009_15/03-GRAMMy/NCBIGenomes06/PR19009_15.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR19009_30/03-GRAMMy/NCBIGenomes06/PR19009_30.tblat.1\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_24/03-GRAMMy/NCBIGenomes06/PR10031_24.tab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_17/03-GRAMMy/NCBIGenomes06/PR10025_17.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_27/03-GRAMMy/NCBIGenomes06/PR10025_27.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_38/03-GRAMMy/NCBIGenomes06/PR10025_38.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10025_9/03-GRAMMy/NCBIGenomes06/PR10025_9.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10029_16/03-GRAMMy/NCBIGenomes06/PR10029_16.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10029_8/03-GRAMMy/NCBIGenomes06/PR10029_8.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_14/03-GRAMMy/NCBIGenomes06/PR10031_14.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_29/03-GRAMMy/NCBIGenomes06/PR10031_29.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10031_8/03-GRAMMy/NCBIGenomes06/PR10031_8.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_19/03-GRAMMy/NCBIGenomes06/PR10047_19.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_27/03-GRAMMy/NCBIGenomes06/PR10047_27.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_39/03-GRAMMy/NCBIGenomes06/PR10047_39.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR10047_9/03-GRAMMy/NCBIGenomes06/PR10047_9.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR19009_15/03-GRAMMy/NCBIGenomes06/PR19009_15.tab\n",
        "Possible empty file for /quakestor2/kertesz/soup/Runs/V2.2/Samples/PR19009_30/03-GRAMMy/NCBIGenomes06/PR19009_30.tab\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blastfi.columns = map(str.lower, blastfi.columns)\n",
      "all_stats.columns = map(str.lower, all_stats.columns)\n",
      "GRAMMyData.columns = map(str.lower, GRAMMyData.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def writeDF(df,s):\n",
      "    inp=df.to_records(index=True)\n",
      "    for i in inp:\n",
      "        q=s%i\n",
      "        try:\n",
      "            conn.execute(q)\n",
      "        except:\n",
      "            None\n",
      "            # print \"Index %s already present.\"%q\n",
      "\n",
      "# Write blast_table\n",
      "blastfi['key']=blastfi['gi']+\"_\"+blastfi['sstart'].astype(str)+\"_\"+blastfi['send'].astype(str)\n",
      "blastfi.columns = map(str.lower, blastfi.columns)\n",
      "s=''' insert into blast_table (gi,sstart,send,taxid,sample_id,name,patient_id,key) values %s '''\n",
      "writeDF(GRAMMyAnnot,s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get Stats and GRAMMy files\n",
      "pipeline='V2.2.0'\n",
      "stats=all_stats.loc[(all_stats['pipeline']==pipeline) & (all_stats['database']==dbHandle),:]    \n",
      "GRAMMyData=GRAMMyData.loc[(GRAMMyData['pipeline']==pipeline) & (GRAMMyData['database']==dbHandle),:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write stat_table\n",
      "stats_data=stats[stats['database']==dbHandle]\n",
      "stats_data.index=stats_data['sample_id']\n",
      "stats_data.reset_index(inplace=True,drop=True)\n",
      "sample_stats_pivot=stats_data.pivot(index='sample_id', columns='stat', values='value')\n",
      "sample_stats_pivot.reset_index(inplace=True,drop=False)\n",
      "sample_stats_pivot['patient_id']=[sam.split('_')[0] for sam in sample_stats_pivot['sample_id']]\n",
      "narrow_stats=sample_stats_pivot[['sample_id','patient_id','chr21_coverage','grammy-blast-count-filtered-count-corrected-NCBIGenomes06']]\n",
      "narrow_stats.columns=['sample_id','patient_id','chr21_coverage','total_blast']\n",
      "narrow_stats.columns = map(str.lower, narrow_stats.columns)\n",
      "narrow_stats.set_index('sample_id',inplace=True)\n",
      "writeDF(narrow_stats,''' insert into stat_table (sample_id,patient_id,chr21_coverage,total_blast) values %s ''')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write sample_table.\n",
      "fi_path='/quakestor2/llmartin/apps/infectome/inputs/patient_data/all_sample_metadata.csv'\n",
      "all_sample_metadata=pd.read_csv(fi_path,index_col=0)\n",
      "all_sample_metadata['Sample_ID']=all_sample_metadata['Patient_ID']+\"_\"+all_sample_metadata['Time']\n",
      "all_sample_metadata.fillna('',inplace=True)\n",
      "samples_annot=all_sample_metadata[['Sample_ID','Patient_ID','Collection_Time','Study','TX_Time']]\n",
      "samples_annot.drop_duplicates(inplace=True)\n",
      "samples_annot['Collection_Time']= pd.to_datetime(samples_annot['Collection_Time'])\n",
      "samples_annot['TX_Time']= pd.to_datetime(samples_annot['TX_Time'])\n",
      "samples_annot['DaysPostTransplant']=(samples_annot['Collection_Time'].sub(samples_annot['TX_Time']))/np.timedelta64(1,'D')\n",
      "samples_annot['DaysPostTransplant']=samples_annot['DaysPostTransplant'].astype(str) # Will need to sort on this\n",
      "# Change this because SQL doesn't like datetime objects\n",
      "samples_annot['Collection_Time'] = samples_annot['Collection_Time'].apply(str)\n",
      "samples_annot['TX_Time'] = samples_annot['TX_Time'].apply(str)\n",
      "samples_annot.columns = map(str.lower, samples_annot.columns)\n",
      "samples_annot.to_sql('sample_table', engine, if_exists='replace')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write clinical_table\n",
      "from dateutil.parser import parse\n",
      "sam_data=samples_annot.set_index('patient_id')\n",
      "sam_data=sam_data['tx_time']\n",
      "sam_data.drop_duplicates(inplace=True)\n",
      "\n",
      "def getTxDate(patient):\n",
      "    \"\"\"Get transplant date.\"\"\"\n",
      "    try: \n",
      "        date=sam_data.loc[patient]\n",
      "    except KeyError: # For some patients, there is no tx_date listed.\n",
      "        date=''\n",
      "    return date\n",
      "\n",
      "fi_path='/quakestor2/llmartin/apps/infectome/inputs/clinical_data/all_clinical_data.csv'\n",
      "clinical_test_data=pd.DataFrame(pd.read_csv(fi_path,index_col=0))\n",
      "clinical_test_data['Index']=clinical_test_data.index\n",
      "clinical_test_data.fillna('',inplace=True)\n",
      "clinical_test_data=clinical_test_data[(clinical_test_data['Date']!='')] \n",
      "clinical_test_data['TX_Date']=clinical_test_data.loc[:,'Patient_ID'].apply(getTxDate)\n",
      "clinical_test_data=clinical_test_data.loc[clinical_test_data['TX_Date']!='',:]\n",
      "clinical_test_data.loc[:,'Date']=clinical_test_data.loc[:,'Date'].apply(parse)\n",
      "clinical_test_data.loc[:,'TX_Date']=clinical_test_data.loc[:,'TX_Date'].apply(parse)\n",
      "delta=clinical_test_data.loc[:,'Date'].subtract(clinical_test_data.loc[:,'TX_Date'])\n",
      "clinical_test_data['DaysPostTX_test']=delta/np.timedelta64(1, 'D')\n",
      "clinical_test_data=clinical_test_data.fillna('')\n",
      "clinical_test_data['Date']=clinical_test_data['Date'].apply(str)\n",
      "clinical_test_data['TX_Date']=clinical_test_data['TX_Date'].apply(str)\n",
      "clinical_test_data.columns = map(str.lower, clinical_test_data.columns)\n",
      "clinical_test_data.to_sql('clinical_table', engine, if_exists='replace')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write pipe_stats_table.\n",
      "path=\"/quakestor2/llmartin/apps/infectome/inputs/pipeline-stats.pickle\"\n",
      "S=pickle.load(open(path,\"r\"))\n",
      "S.index.name='Sample_ID'\n",
      "sam=samples_annot[['sample_id','study']]\n",
      "sam.set_index('sample_id',inplace=True)\n",
      "S_with_cohort=pd.merge(S,sam,left_index=True, right_index=True)\n",
      "S_with_cohort.reset_index(inplace=True,drop=False)\n",
      "S_with_cohort.columns = map(str.lower, S_with_cohort.columns)\n",
      "S_with_cohort.to_sql('pipe_stats_table', engine, if_exists='replace')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write tax_table\n",
      "def makeCompressedTax(taxList):\n",
      "    null=''\n",
      "    compressed=','.join([str(n) for n in taxList if n != null])\n",
      "    return ','+compressed+','\n",
      "path='/quakestor2/llmartin/apps/infectome/inputs/min.tax.table'\n",
      "taxDB='RefseqGenomes'\n",
      "taxTable=pd.DataFrame(pd.read_table(path,dtype=str))\n",
      "taxTable.columns = map(str.lower, taxTable.columns)\n",
      "taxTable.to_sql('tax_table', engine, if_exists='replace')\n",
      "\n",
      "# Add genome size data\n",
      "genomeDF=pd.DataFrame(pd.read_table(\"/quakestor2/kertesz/soup/Data/GenomeDB/%s/grammy/%s.gdt\"%(taxDB,taxDB),header=None,dtype=str))\n",
      "genomeSizes=pd.DataFrame()\n",
      "for element in genomeDF.ix[5,0].strip().split('=')[1].strip()[1:-1].split(','):\n",
      "    taxID,genomeSize=element.strip().split(':')\n",
      "    genomeSizes.loc[str(taxID.strip()),'genome_size']=genomeSize.strip()\n",
      "genomeSizes['tax_id']=genomeSizes.index\n",
      "TaxData = pd.merge(genomeSizes,taxTable,left_on=\"tax_id\", right_on=\"tax_id\")\n",
      "null=''\n",
      "TaxData.fillna(null,inplace=True)\n",
      "TaxData['compressed']=TaxData.loc[:,TaxData.columns[5:-1]].apply(makeCompressedTax,axis=1)\n",
      "taxToMerge=TaxData[['genome_size','tax_id','tax_name','compressed']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GRAMMyData.columns = map(str.lower, GRAMMyData.columns)\n",
      "all_stats.columns = map(str.lower, all_stats.columns)\n",
      "stats.columns = map(str.lower, stats.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Complete GRAMMy data table\n",
      "\n",
      "# Get Stats and GRAMMy files\n",
      "GRAMMyAnnot=pd.merge(GRAMMyData,taxToMerge,left_on=\"tax_id\", right_on=\"tax_id\")\n",
      "\n",
      "# Add blast data\n",
      "blastStat=all_stats.loc[all_stats['stat']=='grammy-blast-count-filtered-%s'%dbHandle,['sample_id','value']]\n",
      "blastStat.columns=['sample_id','blast_count_filtered']\n",
      "GRAMMyAnnot=pd.merge(GRAMMyAnnot,blastStat,left_on=\"sample_id\", right_on=\"sample_id\")\n",
      "\n",
      "# Add mapping data\n",
      "mapStat=stats.loc[stats['stat']=='chr21_coverage',['sample_id','value']]\n",
      "mapStat.columns=['sample_id','chr21_coverage']\n",
      "GRAMMyAnnot=pd.merge(GRAMMyAnnot,mapStat,left_on=\"sample_id\", right_on=\"sample_id\")\n",
      "\n",
      "# Add sample data \n",
      "samAdd=samples_annot[['patient_id','sample_id','daysposttransplant','study']]\n",
      "GRAMMyAnnot=pd.merge(GRAMMyAnnot,samAdd,left_on=\"sample_id\", right_on=\"sample_id\")\n",
      "\n",
      "# Compute absolute and normalized abundance from GRAMMy data\n",
      "GRAMMyAnnot['abundance']=GRAMMyAnnot['abundance'].astype(float)\n",
      "GRAMMyAnnot['genome_size']=GRAMMyAnnot['genome_size'].astype(float)\n",
      "GRAMMyAnnot['abundance_Wt']=GRAMMyAnnot['abundance']*GRAMMyAnnot['genome_size']\n",
      "\n",
      "rec=pd.DataFrame()\n",
      "for sample in set(GRAMMyAnnot['sample_id']):\n",
      "    wt_abun=GRAMMyAnnot.loc[GRAMMyAnnot['sample_id']==sample,'abundance_Wt']\n",
      "    rec.loc[sample,'wt_sum']=wt_abun.sum()\n",
      "rec['sample_id']=rec.index\n",
      "\n",
      "GRAMMyAnnot=pd.merge(GRAMMyAnnot,rec,left_on=\"sample_id\", right_on=\"sample_id\")\n",
      "GRAMMyAnnot['grammyHits']=GRAMMyAnnot['abundance']*GRAMMyAnnot['blast_count_filtered']*GRAMMyAnnot['genome_size']/GRAMMyAnnot['wt_sum']\n",
      "GRAMMyAnnot['normalized_abundance']=GRAMMyAnnot['grammyHits']/GRAMMyAnnot['genome_size']/GRAMMyAnnot['chr21_coverage']\n",
      "GRAMMyAnnot['tax_id']=GRAMMyAnnot['tax_id'].astype(int)\n",
      "GRAMMyAnnot['key']=GRAMMyAnnot['sample_id']+\"_\"+GRAMMyAnnot['tax_id'].astype(str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s=''' insert into grammy_table (sample_id, \ttax_id,abundance,error,pipeline,database,genome_size, \\\n",
      "tax_name,compressed,blast_count_filtered,chr21_coverage,patient_id,daysposttransplant, \\\n",
      "study,abundance_wt,wt_sum,grammyhits,normalized_abundance) values %s '''\n",
      "\n",
      "writeDF(GRAMMyAnnot,s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}