{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Regression: It will compute a real number.\n",
      "* Discriminatative: It will try to learn the output given the features: $P(y \\mid x)$.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* Approximates $y$ as a linear funtion of $x$, our features.\n",
      "* $y = h(x) = \\sum\\limits_{i=0}^n \\theta_i x_i = \\theta^T x $. \n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "        \n",
      "* Model prediction deviates from actual value by an error term: $y^i = \\theta^T x^i + \\epsilon^i$.\n",
      "* Errors are normally distributed: $ P(\\epsilon^i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\frac{-(\\epsilon^i)^{2}}{2\\sigma^2} $\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "    \n",
      "* With substitution (above), we now have the probability of observing the target given our model: $ p(y^i \\mid x^i;\\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(y^i - \\theta^{T}x^i)^2 }{2 \\sigma^2} $\n",
      "* The probability of the training data is the product of the computed target probability: $L(\\theta) = \\prod\\limits_{i=1}^m P(y^{i} \\mid x^i;\\theta) $\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Numerical approaches to find $\\theta$ that maximizes our loglikelihood function:\n",
      "\n",
      "* We have a loglikelihood function, which is convex.\n",
      "* $  l(\\theta) = log L(\\theta) = C + \\sum\\limits_{i=1}^m - \\frac{(y^i - h_{\\theta}(x^i))^2}{2 \\sigma^2} $\n",
      "* We want to find parameter values that maximize the loglikelihood, which best explain the observed training data.\n",
      "* We can use a numerical approach, such as gradient ascent.\n",
      "* $ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $\n",
      "\n",
      "(2) Numerical approaches to find $\\theta$ that minimizes the least squares cost function, $J(\\theta)$:\n",
      "\n",
      "* Minimizing $J(\\theta)$ is the same as maximizing the $l(\\theta)$.\n",
      "* $J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (h_{\\theta} (x^i) - y^i)^2$ \n",
      "* We use a numerical approach, gradient descent or least mean squares (Widrow-Hoff) updater rule.\n",
      "* $ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $\n",
      "\n",
      "(3) Analytical approaches to find $\\theta$ that minimizes the least squares cost function, $J(\\theta)$:\n",
      "\n",
      "* We can compute the gradient of the cost function respect to $\\theta$: $ \\nabla_{\\theta} J(\\theta) $.\n",
      "* We set this equal to zero to obtain normal equations and solve for $\\theta$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Discriminatative: It will try to learn the output given the features: $P(y \\mid x)$\n",
      "        \n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* Approximates $y$ as a logistic funtion of $x$, our features.\n",
      "* $y = h(x) = \\frac{1}{1+e^{-\\theta^T x}} $. \n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "    \n",
      "* $P(y=1 \\mid x;\\theta) = h_{\\theta}(x) $   \n",
      "* $P(y=0 \\mid x;\\theta) = 1- h_{\\theta}(x) $ \n",
      "* Combing the above, $P(y \\mid x;\\theta) = (h_{\\theta}(x))^y (1 - h_{\\theta}(x))^{1-y}$\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "    \n",
      "* $ L(\\theta) = \\prod\\limits_{i=1}^m (h_{\\theta}(x^i))^{y^i} (1 - h_{\\theta}(x^i))^{1-y^i} $\n",
      "* $ l(\\theta) = log L(\\theta) = \\sum\\limits_{i=1}^m y^i \\times log h_{\\theta}(x^i) + (1-y^i) \\times log(1- h_{\\theta}(x^i)) $\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Gradient ascent, one numerical approach to find $\\theta$ that maximizes our loglikelihood function:\n",
      "\n",
      "* The general gradient ascent update rule: $ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $\n",
      "* This is solved, $y^i - (h_{\\theta} (x^i)) x_j^i$\n",
      "* $ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $\n",
      "\n",
      "(1) Newton's method, another numerical approach to find $\\theta$ that maximizes our loglikelihood function:\n",
      "\n",
      "* Enjoys faster convergence than (batch) gradient descent.\n",
      "* $\\theta := \\theta - H^{-1} \\nabla l(\\theta)$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Perceptron algorithm - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Discriminatative: It will try to learn the output given the features: $P(y \\mid x)$\n",
      "        \n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* We define a threshold function, $g(z)$.\n",
      "* $ h_{\\theta}(z) = g(z) $ where $z = \\theta^T x $.\n",
      "* 1 if $z$ $\\geq$ 0 \n",
      "* 0 if $z$ < 0\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "\n",
      "* It is difficult to endow this algorithm with meaningful probabilistic interpretations.\n",
      "* Also, it is difficult to or derive the perceptron as a maximum likelihood estimation algorithm.\n",
      "    \n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "    \n",
      "* None.\n",
      "    \n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "A numerical approach, which is a stochastic gradient descent-like implementation.\n",
      "\n",
      "* The perceptron algorithm will only make one pass through the entire training set.\n",
      "* It uses a gradient descent-like implementation of the updater rule: $ \\theta^{i+1} := \\theta^i + \\alpha[y^{i+1} - h_{\\theta}(x^{i+1})]x^{i+1} $.\n",
      "* We know that $h_{\\theta}(z) = g(z) = g(\\theta^T x) $.\n",
      "* We simplified this problem using Kernels.\n",
      "* We first re-wrote $\\theta^i=\\sum\\limits_{l=1}^i \\beta_l \\phi(x^l)$ where $\\beta_l = \\alpha(y^{l} - h_{\\theta}(\\phi(x^l))$.\n",
      "* When we update to compute $\\theta_{i+1}$ for each training example $x_{i+1}$, we use $\\theta_{i}$.\n",
      "* $ h_{\\theta}(x_{i+1}) = g(\\theta_i^T x_{i+1}) = g(\\theta_i^T x_{i+1}) = g(\\sum\\limits_{l-1}^i \\beta_l \\phi(x^l)^T \\phi(x_{i+1})) = g(\\sum\\limits_{l-1}^i \\beta_l K(\\phi(x^l),\\phi(x_{i+1})) $\n",
      "* Thus, we re-wrote our hypothesis in terms of a Kernel function, which can be efficiently computed.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generalized linear models - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Preface} -$\n",
      "\n",
      "* We have hypothesis functions for linear, logistic regression that relatate target to features: $h_{\\theta}(x) = y = f(\\theta,x)$ \n",
      "* Likewise, we have a probabilistic interpretation of the computed computed target values $h_{\\theta}(x)$.\n",
      "* The probabilistic interpretation allows us to write a likelihood function, $L(\\theta) = \\prod\\limits_{i=1}^m P(y^{i} \\mid x^i;\\theta) $.\n",
      "* The likelihood function includes our training data.\n",
      "* By maximizing the likelihood function on our training data, we learn parameters the more likely paramters $\\theta$.\n",
      "\n",
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* GLMs allow us to derive a hypothesis function based upon a large class of (exponential family) probability distributions.\n",
      "* For something like customer arrivals (or page-views), we might want a model underpinned by a Poisson distribution. \n",
      "* In turn, the model can either be classification or regression.\n",
      "        \n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* Exponential family distributions can be written in the form: $P(y ; \\eta) = b(y) exp ( \\eta^T T(y) - a(\\eta) )$\n",
      "* Starting with a familiar probabilistic model and parameters (e.g., below for Bernoulli):\n",
      "* $P(y;\\phi) = \\phi^y (1-\\phi)^{1-y}$ with $E[y \\mid x]=\\phi$.\n",
      "* We can re-write this in conventional exponential family form.\n",
      "* In turn, we can re-write the model parameters in terms of the exponential parameter $\\eta$.\n",
      "* $\\phi = f(\\eta)$ where, by definition, $\\eta=\\theta x$.\n",
      "* In turn, we want our hypothesis to simply be the expected value.\n",
      "* In turn, we derive hypothesis function: $E[y \\mid x]=\\phi=f(\\eta)=f(\\theta x) =h_{\\theta}(x)$.\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "\n",
      "* The model follows the assumpotions of the chosen probability distribution!\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "    \n",
      "* The chosen probability distribution has a native probability function: $P(y;\\phi) = \\phi^y (1-\\phi)^{1-y}$\n",
      "* We simply used the GLM to re-write the model parameters in terms of our training data input features and paramters. \n",
      "* The probability function is re-written in terms of features and paramters: $P(y;\\phi) = h_{\\theta}(x)^y (1-h_{\\theta}(x))^{1-y}$\n",
      "* Then we can simply maximize the loglikelihood, as we did previously!\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gaussian discriminant analysis - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Generative: It will try to learn the features, given the label, $P(x \\mid y)$.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* The model uses a multivariate Gaussian to model the features given a class label.\n",
      "* $ P(x \\mid y_o) = \\frac{1}{\\Sigma^{0.5} (2 \\pi)^{0.5n} } exp -\\frac{ (x-\\mu_o) (x-\\mu_o)^T }{2 \\Sigma} $\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "        \n",
      "* The class features can be modeled as Gaussian.\n",
      "* The covariance matrix ($\\Sigma$), which is essentialy the shape of the distribution, is shared between classes.\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* We define a joint loglikelihood function, as we did previously! \n",
      "* $l(\\phi,\\mu_o,\\mu_1,\\Sigma) = \\sum_{i=1}^m log (P(x \\mid y)) p(y)$.\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter.\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* Previously, we had a hypothesis function with respect to our trained parameters that we could on the test data.\n",
      "* Now, we train maximum likelihood estimates of each parameter on data for each class, giving us $\\phi$, $\\mu_o$, $\\mu_1$, $\\Sigma$.\n",
      "* For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule:\n",
      "* $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $\n",
      "* To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayes - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Generative: It will try to learn the features, given the label, $P(x \\mid y)$.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* $P(X \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "* $\\phi_{i|y=1} = p(x_i = 1|y = 1)$\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "  \n",
      "* For GDA, we defined a multinomial Gaussian such that each combination of features had a unique probability assigned to it!\n",
      "* But, that doesn't scale well to cases of many features (e.g., text classification with a large dictionary).\n",
      "* Here, we assume the occournace of every feature is conditionally independent given $y$.\n",
      "* That is the probability of observing the words \"buy\" and \"Viagra\" in a spam e-mail are independent.\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* $ L = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Analytical approaches to find model parameters:\n",
      "\n",
      "* In this case, took the derivative of the likelihood function with respect to each and set equal to zero.\n",
      "* This allowed us to derive formulas for the maximum likelihood estimates of each parameter (e.g., \\phi_{i|y=1}).\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* This is the same as GDA!\n",
      "* We train maximum likelihood estimates of each parameter on data for each class.\n",
      "* For test data, we then compute the posterior probability that it belongs to each class, using Bayes rule:\n",
      "* $ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $\n",
      "* To assign the test example, $X$, we simply compute the posterior probability for each of the classes and pick the better one!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVMs - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Algorithm type} -$\n",
      "\n",
      "* Classification: It will compute a class label.\n",
      "* Discriminatative.\n",
      "\n",
      "$\\textbf{Model} -$\n",
      "\n",
      "* Classes for $y \\in \\{-1,1\\}$.\n",
      "* $h(z) = g(z)$ where $g(z)$ is a yet undefined step-funtion.\n",
      "* $z=w^Tx + b$ is a linear combination of features and parameters.\n",
      "* If $y^i=1$, we train to generate $w^Tx + b >> 0$ and our classifier picks 1.\n",
      "\n",
      "$\\textbf{Assumptions} -$\n",
      "\n",
      "$\\textbf{Likelihood estimate} -$   \n",
      "\n",
      "* We did not use a probabilistic framing for SVMs.\n",
      "\n",
      "$\\textbf{Strategies for training our model parameters} -$  \n",
      "\n",
      "(1) Use convex optimization:\n",
      "\n",
      "* Our objective is to maximize the geometric margin.\n",
      "* We set this up as an optimization problem and set it up in dual form.\n",
      "* $f(w) = \\|w\\|^{2}$\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $ with $g_i(w) \\leq 0$ for the optimal margin (meaning all points are correctly classified).\n",
      "* $ L(w,b,\\alpha) = \\|w\\|^{2} - \\sum\\limits_{i = 1}^{k} \\alpha_i [y^i (w^T x^i + b) -1] $\n",
      "* In the dual formulation, we first minimize the Lagrangian with respect to $\\omega$: $ \\theta(\\alpha,\\beta) = min_{\\omega,b} L(\\omega,b,\\alpha) $\n",
      "* This leads to the optimal parameter value, $\\omega$.\n",
      "* We further use this to find the values of $\\alpha$: $ max_{\\alpha; \\alpha \\geq 0} W(\\alpha)$.\n",
      "\n",
      "$\\textbf{Implementing the model} -$\n",
      "\n",
      "* Now, if we want to make a prediction for a new point, x: \n",
      "* $ \\omega^T x + b = (\\sum_{i=1}^m \\alpha_i y^i x^i)^T x + b = \\sum_{i=1}^m \\alpha_i y^i <x^i x> + b $\n",
      "* Where $\\alpha_i > 0$ only for points closest to the decision boundary (the support vector)!\n",
      "* Further, it involoves an inner product, meaning that we can Kernelize it to make this computation efficient.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Misc items - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(1) Softmax regression\n",
      "\n",
      "(2) Multi-variate Bernoulli event model\n",
      "\n",
      "(3)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}