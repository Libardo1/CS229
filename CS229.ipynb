{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt \n",
      "import pandas as pd\n",
      "import re\n",
      "import cmath\n",
      "import math\n",
      "import glob\n",
      "import subprocess\n",
      "import scipy.stats as stats\n",
      "import pylab as pl\n",
      "from scipy.spatial.distance import pdist, squareform\n",
      "from scipy.cluster.hierarchy import linkage, dendrogram\n",
      "from sklearn import svm, datasets\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Intro to ML- "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Definition - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Arthur Samuel on Machine learning from 1959: \n",
      "\n",
      "\"Field of study that gives computers the ability to learn without being explicitly programmed.\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Supervised learning - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Teach the computer how to do something. Apply this learning to new problems:\n",
      "\n",
      "* Classification problems\n",
      "* Reccomender systems\n",
      "* Identification of tumor type by expression profile\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Unsupervised learning - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let the computer learn how to do something, and use this to determine structure and patterns in data:\n",
      "\n",
      "* Clustering problems\n",
      "* Classifying sub-populations of single cells\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Supervised learning-"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Regression versus classification - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The objective of supervised learning is to $learn$ a function, $h :x \\mapsto y $, so that $h(x)$ is a good predictor of $y$. \n",
      "\n",
      "$x$ is often used to denote the input variables (features) whereas $y$ is the output (target). \n",
      "\n",
      "For a simple regression problem: \n",
      "\n",
      "* 13 attributes of housing markets around Boston along with median house price. \n",
      "\n",
      "* It is possible to predict the price of a market given its attributes?\n",
      "\n",
      "First, we examine the target data - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()\n",
      "plt.hist(boston.target)\n",
      "plt.xlabel('price ($1000s)')\n",
      "plt.ylabel('count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<matplotlib.text.Text at 0x109339f10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEPCAYAAACp/QjLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1091b7b90>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Feature selection}$ is a simple, manual way to evaluate the relationship between the features and target - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i=0\n",
      "crime=boston.data[:,i]\n",
      "plt.scatter(crime,boston.target)\n",
      "plt.axis('tight')\n",
      "plt.xlabel('%s'%boston.feature_names[i])\n",
      "plt.ylabel('Price ($1000s)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<matplotlib.text.Text at 0x109392d90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEKCAYAAADzQPVvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9oIgCDLw/7/osyvPhBKIAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10933ead0>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Key concepts for linear and logistic regression - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear regression -"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "We expression our hypothesis as a linear funtion of our features - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We ultimatly want a funtion, $h$, to predict the target value.\n",
      "\n",
      "$\\textbf{Linear regression}$ approximates $y$ as a linear funtion of $x$, our features: \n",
      "\n",
      "$y = h(x) = \\sum\\limits_{i=0}^n \\theta_i x_i = \\theta^T x $. \n",
      "\n",
      "Note that this funtion has an intercept term, $\\theta_o$, that we are leaving out of this definition.\n",
      "\n",
      "This an an example of a $\\textbf{discriminatative}$ learning algorithm:\n",
      "\n",
      "* It will try to learn the output given the features: $P(y \\mid x)$.\n",
      "\n",
      "The challenge is to us our $m$ training examples in order to parameterize $\\theta^T$ in the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use linear regression package.\n",
      "from sklearn.linear_model import LinearRegression\n",
      "clf = LinearRegression()\n",
      "\n",
      "# When we fit the observed features to target, we determine the coefficients used in our linear funtion or model. \n",
      "clf.fit(boston.data,boston.target)\n",
      "# Pass the features for each point to the funtion, resulting in a prediction.\n",
      "predicted=clf.predict(boston.data)\n",
      "# Examine how well the prediction matches the data.\n",
      "plt.scatter(boston.target,predicted)\n",
      "plt.plot([0, 50], [0, 50], '--k')\n",
      "plt.axis('tight')\n",
      "plt.xlabel('True price ($1000s)')\n",
      "plt.ylabel('Predicted price ($1000s)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<matplotlib.text.Text at 0x1091c1cd0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEKCAYAAADzQPVvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA+6C0bKr2HL+AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10935e150>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "A cost funtion is used to determine $\\theta$ in our linear regression model - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One strategy for solving this problem is to find $\\theta$ that makes our prediction as close as possible to the target across our training examples.\n",
      "\n",
      "Specifically, we can define a $\\textbf{cost funtion}$ that chooses coefficients $\\theta$ such that $h(x)$, our prediction, is close to the target, $y$:\n",
      "\n",
      "$J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (h_{\\theta} (x^i) - y^i)^2$\n",
      "\n",
      "To be clear, $h_{\\theta}$ is the linear model parameterized by the coefficients $\\theta$.\n",
      "\n",
      "Thus, we can use this to find $\\theta$ that will minimize the squared distance (or squared error) between the prediction and the target.\n",
      "\n",
      "This is the least squares cost funtion that gives rise to the $\\textbf{ordinary least squares}$ regression model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The cost funtion is convex - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because the cost funtion is quadratic, it's easy to convince yourself that it's also convex.\n",
      "\n",
      "It will have a single global minimum.\n",
      "\n",
      "This is relevant, because it allows us to define an iterlative algorithm guaranteed to find the minimum.\n",
      "\n",
      "For example, set up a toy cost funtion and vary one of the parameters - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Theta params\n",
      "t_o=4\n",
      "t_2=3\n",
      "\n",
      "# Features\n",
      "x_1=1\n",
      "x_2=2\n",
      "y_o=10\n",
      "\n",
      "def computeT(t_1):\n",
      "    # Linear model\n",
      "    h_theta=t_2*x_2+t_1*x_1+t_o\n",
      "    # Cost funtion\n",
      "    j_theta=(h_theta-y_o)**2\n",
      "    return j_theta\n",
      "\n",
      "# Sweep param\n",
      "t_1=np.linspace(-10,10,100)\n",
      "plt.scatter(t_1,computeT(t_1))\n",
      "plt.ylabel('J(Theta)')\n",
      "plt.xlabel('Theta_1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<matplotlib.text.Text at 0x10934b690>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEQCAYAAACqduMIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQRZICIYQQokhSIIQQQhTp/wEEYI+ju8jotwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1093d4f50>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As seen above, the change in $\\frac{\\partial}{\\partial \\theta_1}$ will be negative for all values except the minimum, $\\theta_1=0$.\n",
      "\n",
      "Of course, it forms a convex surface if all paramters are varied. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradient descent, an algorithm for finding $\\theta$ that minimizes the cost funtion - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we need a search algorithm to find $\\theta$ that will minimize squared error and converge to the local minimum.\n",
      "\n",
      "Gradient descent is one algorithm for doing this.\n",
      "\n",
      "It starts with a guess for $\\theta$ and iterativly changes $\\theta$ in the direction of greatest descent of $J(\\theta)$.\n",
      "\n",
      "With $\\alpha$ as the learning rate and := indicating assignment of values (as opposed to assertion of equality), for any element $j$ of $\\theta$:\n",
      "\n",
      "$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $ where the partial derivative of $J(\\theta)$ with respect to $\\theta_j$ indicates the direction of steepest descent of the function.\n",
      "\n",
      "Using the above plot, we can see that is inutitive and will update $\\theta$ in the correct direction for values greater / less than the minimum."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Derivation - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can derive Gradient descent analytically, starting with the cost funtion: \n",
      "    \n",
      "$$J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (h_{\\theta} (x^i) - y^i)^2$$\n",
      "\n",
      "Lets neglect the summation, so we will show this for one training example:\n",
      "    \n",
      "$$J(\\theta) = \\frac{1}{2} (h_{\\theta} (x^i) - y^i)^2$$\n",
      "\n",
      "We compute the partial with respect to one of our parameters, $\\theta_j$:\n",
      "\n",
      "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2} (h_{\\theta} (x) - y)^2 $$\n",
      "\n",
      "$$ = (h_{\\theta} (x) - y) \\frac{\\partial}{\\partial \\theta_j} (\\sum\\limits_{j=1}^m \\theta_j (x_j)) - y^i $$\n",
      "\n",
      "$$ = (h_{\\theta} (x) - y) x_j $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supplemental reading on this:\n",
      "    \n",
      "* http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
      "    \n",
      "* http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html\n",
      "    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The least mean squares (Widrow-Hoff) learning rule - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This gives us the $\\textbf{least mean squares (LMS) updater rule}$:\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n",
      "\n",
      "$$ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $$\n",
      "\n",
      "To be clear, $j$ indicates a specific feature.\n",
      "\n",
      "In practice, this is performed for all values of $j$ simultanously: $x$ and $\\theta$ are vectorized when we actually run this computationally. \n",
      "\n",
      "This definition also assumes a single training example, ($x^i$, $y^i$).\n",
      "\n",
      "Intuitivly, we see:\n",
      "\n",
      "* The magnitude of the update is proportional to the error term: $y^i - (h_{\\theta} (x^i)$.\n",
      "\n",
      "* $J(\\theta)$ is convex, quadratic with one global minimum, so gradient descent - in this case - is used to minimize a quadratic function.\n",
      "\n",
      "Note that we can also run this in batch, meaning that we compute the update for all training examples before taking another step:\n",
      "    \n",
      "$$ \\theta_j := \\theta_j + \\alpha  \\sum\\limits_{i=1}^m (y^i - h_{\\theta} (x^i)) x_j^i $$\n",
      "\n",
      "Alternativly, we can choose stochastic gradient descent, which means we update $\\theta$ and take a step after processing a single training example."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example of stochastic gradient descent - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is an implementation of stochastic gradient descent.\n",
      "\n",
      "def lms(data,eta,iterations):\n",
      "\n",
      "    # Get the number of attributes/features\n",
      "    target = data[1]\n",
      "    data = data[0]\n",
      "    dim = data.shape[1] \n",
      "    num_points = data.shape[0]\n",
      "    \n",
      "    # Starting guess for theta.\n",
      "    w = np.ones(dim) # that is the weight vector as column vector   \n",
      "    print \"Starting theta: %s\"%w\n",
      "\n",
      "    # Number of training examples to sample.\n",
      "    for i in range(iterations):\n",
      "        \n",
      "        # Pick a sample from the training data.\n",
      "        next = np.random.randint(num_points)  \n",
      "        \n",
      "         # Make a prediction based upon the current theta (w).\n",
      "        predict = np.dot(data[next,],w) \n",
      "        \n",
      "        # Determine the error.\n",
      "        error = target[next]-predict       \n",
      "        \n",
      "        # Increment theta: training example * error * step size + pre-existing guess.\n",
      "        w = w + eta*error*data[next,]  \n",
      "        \n",
      "    print \"Theta after %s iterations: %s\"%(iterations,w)\n",
      "    return w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set of training examples.\n",
      "m=10\n",
      "# Number of features and the actual coefficient used to produce target data.\n",
      "f=1\n",
      "theta=0.2\n",
      "toy=np.arange(m).reshape((m,f))\n",
      "# Make target data\n",
      "target=toy*theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set iterations used for stochastic gradient descent.\n",
      "it=20\n",
      "eta=.05\n",
      "plt.subplot(1,2,1)\n",
      "data_zipped=(toy,target)\n",
      "w=lms(data_zipped,iterations=it,eta=eta)\n",
      "plt.plot(toy,target,'g.',markersize=20,alpha=0.5)\n",
      "plt.plot(toy,np.dot(toy,w),'r.',markersize=20,alpha=0.5)\n",
      "plt.show()\n",
      "data_zipped = (toy,target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting theta: [ 1.]\n",
        "Theta after 20 iterations: [ 0.28665295]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAALsAAAEACAYAAADr6gdoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmsGwk2Yw7KQZDDtpxv8DaW/31toombQAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1093c2750>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Useful tricks for gradient descent - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(1) Feature scaling\n",
      "\n",
      "* Mean normalization can be used to ensure all features have a mean value of zero: $x_1=\\frac{x_1 - \\mu_1}{\\sigma_1}$\n",
      "\n",
      "(2) Alpha\n",
      "\n",
      "* Enesure the step size is sufficiently small so that the funtion will converge."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "A closed form solution for $\\theta$ that minimizes the cost funtion - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rather than using an iterative algorithm, we can also try to find a closed-form solution for $\\theta$ that minimizes $J(\\theta)$.\n",
      "\n",
      "With this in mind, let's re-write J($\\theta$).\n",
      "\n",
      "First, we define the design matrix, $X$, that contains traning example input values as rows. \n",
      "\n",
      "We also define a vector, $y_{tar}$, with the target values.\n",
      "\n",
      "J($\\theta$) can be re-cast in terms of $X$ and $y$: \n",
      "\n",
      "$$ z = X\\theta - y_{tar} $$\n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (z_i)^2 = \\frac{1}{2} (z)^T (z) $$\n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (x^i\\theta - y^i)^2 = \\frac{1}{2} (X\\theta - y_{tar})^T (X\\theta - y_{tar}) $$\n",
      "\n",
      "Because we know that the cost function is convex (e.g., there is one global minimum), we can: \n",
      "\n",
      "* Proove that it is convex by showing that the Hessian is positive semi-definite, as we did with the loglikelihood function.\n",
      "* Use an iterative algorithm (e.g., gradient descent or Newton's method) in order to find $\\theta$ that minimizes $J(\\theta)$.\n",
      "* Find a closed-form solution for $\\theta$ by taking the derivative, setting it equal to zero, and solving for $\\theta$. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to compute a closed-form solution, we must apply matrix derivates. Recall the gradient of a matrix:\n",
      "    \n",
      "\\begin{equation} (\\nabla_A f(A))_{ij} = \\frac{\\partial(A)}{\\partial(A_{ij})} \\end{equation}\n",
      "\n",
      "Given a matrix:\n",
      "\n",
      "\\begin{equation} A= \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\end{equation}\n",
      "\n",
      "And a funtion applied to the matrix that returns a scalar:\n",
      "\n",
      "\\begin{equation} f(A) = \\frac{3}{2}A_{11} + 5A_{12}^2+A_{21}A_{22} \\end{equation}\n",
      "\n",
      "Then the gradient is simply the partial derivative of the funtion with respect to each element in the matrix:\n",
      "\n",
      "\\begin{equation} \\nabla_A f(A) = \\begin{bmatrix} \\frac{3}{2} & 10A_{12} \\\\ A_{22} & A_{21} \\end{bmatrix} \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this in mind, we first re-cast J($\\theta$)  in terms of $X$ and $y$: \n",
      "\n",
      "$$ z = X\\theta - y_{tar} $$\n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (z_i)^2 = \\frac{1}{2} (z)^T (z) $$\n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (x^i\\theta - y^i)^2 = \\frac{1}{2} (X\\theta - y_{tar})^T (X\\theta - y_{tar}) $$\n",
      "\n",
      "We compute the derivative and simplify (see lecture notes):\n",
      "\n",
      "* $ \\nabla_{\\theta} J(\\theta) $\n",
      "* We distribute terms and proceed.\n",
      "* The key and rather obscure trick is that we use the following identity to simplify our expression: \n",
      "* $ \\nabla trABA^TC=B^TA^TC^T +BA^TC $\n",
      "\n",
      "After using the $tr$ trick, we end up with a simple expression:\n",
      "\n",
      "$$ \\nabla_{\\theta} J(\\theta) = X_T X \\theta - X^T y_{tar} $$\n",
      "\n",
      "Set this to zero in order get the value of $\\theta$ at which there's no change in the cost funtion, indicating minimization:\n",
      "\n",
      "$$ \\theta = (X_T X)^{-1} X^T y_{tar} $$\n",
      "\n",
      "The challenge with this approach is that it does not always work; there is a not a closed-form solution for every algorithm. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Non-parametric linear regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above examples represent a parametric learning algorithm.\n",
      "\n",
      "We fit a fixed set of parameters $\\theta$, discard the training set, and use the obtained funtion $h_{\\theta}$. Memory requirements only require keeping $\\theta$.\n",
      "\n",
      "In contrast, in a non-parametric learning algorithm we need to keep the full training set. \n",
      "\n",
      "In turn, memory requirments grows (linearly) as a funtion of the training set size. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Locally weighted linear regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to make a prediction at a specific location, x. \n",
      "\n",
      "Locally weighted linear regression will look in the vicinity of x and use those $local$ examples to define $h_{x}$.\n",
      "\n",
      "That is why we need to keep the training data in memory!\n",
      "\n",
      "In this case, the cost funtion is similar to what we had previously: $J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m w^i (h_{\\theta} (x^i) - y^i)^2$\n",
      "\n",
      "But it includes the term $w^i$, which will discount training examples from the point, $x$, at which we want to make a prediction.\n",
      "\n",
      "$w^i = exp [-\\frac{(x^i - x)^2}{2}]$ and can include $\\tau^2$, which can influence the number of points we include.\n",
      "\n",
      "Thus, if $x^i - x$ is large $w^i \\approx 0$ and the training example, $x^i$ will not be included in determination of $\\theta$.\n",
      "\n",
      "We implement in this homework #1.\n",
      "\n",
      "The downside is that we re-fit the model every time we make a prediction for a given value of $x$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifically, we can write \n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} (X\\theta - y_{tar})^T W (X\\theta - y_{tar}) = W \\sum\\limits_{i=1}^m (x^i\\theta - y^i)^2 $$\n",
      "\n",
      "Where $ W = \\frac{1}{2}w^i $.\n",
      "\n",
      "We can generalize the normal equations to this weighted setting, starting with:\n",
      "\n",
      "$$ J(\\theta) = \\frac{1}{2} (WX^T X \\theta^T \\theta - WX^T \\theta^T y - WX \\theta y^T + Wy y^T) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We take the derivative and solve for $\\theta$:\n",
      "\n",
      "$$ \\nabla J(\\theta) = \\nabla \\frac{1}{2} tr(WX^T X \\theta^T \\theta - WX^T \\theta^T y - WX \\theta y^T + Wy y^T) $$\n",
      "\n",
      "$$ \\nabla J(\\theta) = \\nabla \\frac{1}{2} (trWX^T X \\theta^T \\theta - 2trWX \\theta y^T) $$\n",
      "\n",
      "Note that:\n",
      "\n",
      "* We can combine terms because $trA = trA^T$.\n",
      "* The constant term drops because we're taking the derivative with respect to $\\theta$. \n",
      "\n",
      "With the terms:\n",
      "\n",
      "* $A^T = \\theta$\n",
      "* $B=B^T=X^TX$ \n",
      "* $C=I$\n",
      "\n",
      "$$ \\nabla J(\\theta) = \\nabla \\frac{1}{2} (W tr B A A^T - 2trWX A^T y^T) $$\n",
      "$$ \\nabla J(\\theta) = \\nabla \\frac{1}{2} (W tr (A B A^T C) - 2Wy^T  tr (X A^T)) $$\n",
      "\n",
      "In the notes, they use: \n",
      "\n",
      "* $ \\nabla trABA^TC = B^TA^TC^T +BA^TC $.\n",
      "* So, $ tr (A B A^T C) = B^T A^T C^T + B A^T C $. \n",
      "\n",
      "Also, in the notes they use:\n",
      "\n",
      "* $ \\nabla trAB = B^T $.\n",
      "* $ tr (X A^T) = tr (B? A^T) = B^T = X^T $\n",
      "\n",
      "$$ \\nabla J(\\theta) = \\frac{1}{2} W(X X^T \\theta + X^T X \\theta - 2Wy X^T) $$\n",
      "$$ \\nabla J(\\theta) = \\frac{1}{2} (W X X^T \\theta + W X^T X \\theta - 2 W y X^T) $$\n",
      "$$ \\nabla J(\\theta) = (W X X^T \\theta - W y X^T) $$\n",
      "\n",
      "Therefore, the minimum value of $\\theta$ is observed at:\n",
      "\n",
      "$$0 = W X X^T \\theta - W y X^T$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Probabilistic interpretation of linear regression - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Maximum likliehood estimation is a probabilistic framing of the previously defined cost funtion - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why do we choose to minimize the squared error?\n",
      "\n",
      "Let's make some assumptions:\n",
      "\n",
      "(1) Assume that housing pricing are a linear funtion of the various features along with an error term, which includes noise (e.g., emotions that affect transaction price): \n",
      "\n",
      "$y^i = \\theta^T x^i + \\epsilon^i$.\n",
      "\n",
      "(2) Assume noise is distributed according to gaussian with $\\mu=0$ and variance $\\sigma^2$. \n",
      "\n",
      "(3) Assume that every housing transaction suffers from this noise and that noise in each transaction is independent. \n",
      "\n",
      "The general probability density funtion for a normal distribution with mean, $\\mu$, and standard deviation, $\\sigma$, is:\n",
      "\n",
      "$$ P(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(-\\frac{(x-\\mu)^{2}}{2\\sigma^2}) $$\n",
      "\n",
      "The probability density funtion of $\\epsilon^i$ is given below, assuming $\\mu = 0$:\n",
      "\n",
      "$$ P(\\epsilon^i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\frac{-(\\epsilon^i)^{2}}{2\\sigma^2} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The difference between the prediction and the actual values is normally distrubted. \n",
      "\n",
      "We are saying that there is a distribution of outcomes, $y^i$, given $x_i$ and parameterized by $\\theta$.\n",
      "\n",
      "The expected value of this distribution is simply the guess dervied from our model, $\\theta^T x^i$.\n",
      "\n",
      "Therefore, substituting for $\\epsilon$ using our definition ($y^i = \\theta^T x^i + \\epsilon^i$) above:\n",
      "\n",
      "$$ p(y^i \\mid x^i;\\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(y^i - \\theta^{T}x^i)^2 }{2 \\sigma^2} $$\n",
      " \n",
      "Thus, there is a normal distrubtion of values, $y$, for any training example $x$ parameterized by $\\theta$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can re-cast this as a funtion of $\\theta$. This is the liklihood funtion, meaning the liklihood of $\\theta$ given our training data.\n",
      "\n",
      "$$L(\\theta) = P(y_{tar} \\mid X;\\theta) $$\n",
      "\n",
      "We then want to find $\\theta$ that maximizes the probability of observing our target prices given the observed features. \n",
      "\n",
      "This is simply the product of the computed target probability (given features parameterized by $\\theta$) for all training data: \n",
      "\n",
      "$$L(\\theta) = P(y_{tar} \\mid X;\\theta) = \\prod\\limits_{i=1}^m P(y_{i} \\mid x^i;\\theta) $$\n",
      "\n",
      "Find $\\theta$ that maximizes the liklihood of the data:\n",
      "\n",
      "$$  l(\\theta) = log L(\\theta) = log \\prod\\limits_{i=1}^m P(y_{i} \\mid x^i;\\theta) = m \\times log \\frac{1}{\\sqrt{2 \\pi \\sigma}} + \\sum\\limits_{i=1}^m - \\frac{(y^i - \\theta^{T}x^i)^2}{2 \\sigma^2} = m \\times log \\frac{1}{\\sqrt{2 \\pi}\\sigma} + \\sum\\limits_{i=1}^m - \\frac{(y^i - h_{\\theta}(x^i))^2}{2 \\sigma^2} $$\n",
      "\n",
      "Thus, maximizing $L(\\theta)$ is the same as minimizing our initial least squares cost funtion, $J(\\theta) = \\frac{1}{2} \\sum\\limits_{i=1}^m (h_{\\theta} (x^i) - y^i)^2$.\n",
      "\n",
      "Furthermore, we now see that the Gaussian assumption underpins the previously defined least squares model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Along these lines, we can also define the MAP (maximum a posteriori probability) for Bayesian linear regression using a Gaussian prior:\n",
      "\n",
      "$$ \\theta_{MAP} = argmax_{\\theta} (\\prod\\limits_{i=1}^m P(y_{i} \\mid x^i;\\theta)) p(\\theta) = argmax_{\\theta} (\\sum\\limits_{i=1}^m log (P(y_{i} \\mid x^i;\\theta))) + log (p(\\theta)) $$\n",
      "\n",
      "The probability density funtion for the prior, $p(\\theta)$, is normal with $\\mu = 0$ and standard deviation, $\\tau$:\n",
      "\n",
      "$$ P(\\theta^i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\frac{-(\\theta^i)^{2}}{2\\tau^2} $$\n",
      "\n",
      "The probability density funtion for $P(y^{i} \\mid x^i;\\theta)$:\n",
      "\n",
      "$$ p(y^i \\mid x^i;\\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(y^i - \\theta^{T}x^i)^2 }{2 \\sigma^2} $$\n",
      "\n",
      "$$ \\theta_{MAP} = argmax_{\\theta} (\\sum\\limits_{i=1}^m  \\frac{ -(y^i - \\theta^{T}x^i)^2 }{2 \\sigma^2} + \\frac{-(\\theta^i)^{2}}{2\\tau^2} ) = argmax_{\\theta} ( \\frac{ - (Y - \\theta^{T}X)(Y - \\theta^{T}X)^T }{2 \\sigma^2} + \\frac{-(\\theta \\theta^T )^{2}}{2\\tau^2} ) $$\n",
      "\n",
      "$$ \\theta_{MAP} = argmin_{\\theta} ( \\frac{ (Y - \\theta^{T}X)(Y - \\theta^{T}X)^T }{2 \\sigma^2} + \\frac{\\theta \\theta^T}{2\\tau^2} ) $$\n",
      "\n",
      "$$ \\theta_{MAP} = argmin_{\\theta} ( (Y - \\theta^{T}X)(Y - \\theta^{T}X)^T + \\frac{\\sigma^2 \\theta \\theta^T}{\\tau^2} ) $$\n",
      "\n",
      "At this point, take the derivative ($\\nabla$), set equal to zero, and solve for $\\theta_{MAP}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic regression-"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Logistic regression is a classification algorithm - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unlike linear regression, logistic regression is a classication algorithm.\n",
      "\n",
      "This is useful, because linear regression will often fail on training data with a categorical (binary) target.\n",
      "\n",
      "Assuming we have two classes, with $y \\in \\{0,1\\}$.\n",
      "\n",
      "For linear regression, we defined a funtion $h_{\\theta}(x)$ that was linear. \n",
      "\n",
      "For logistic regression, we simply enforce that $h_{\\theta}(x)$ outputs values between 0 and 1 and use the logistic funtion.\n",
      "\n",
      "We define a set of probabilistic assumptions for our model, as shown below:\n",
      "\n",
      "$$ P(y=1 \\mid x;\\theta) = h_{\\theta}(x) = g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^T x}}$$\n",
      "\n",
      "$$ P(y=0 \\mid x;\\theta) = 1 - h_{\\theta}(x) $$ \n",
      "\n",
      "This is written compactly as:\n",
      "\n",
      "$$ P(y \\mid x;\\theta) = h_{\\theta}(x)^y (1-h_{\\theta}(x))^{1-y} $$ "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Logistic regression decision boundary - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given $h_{\\theta}(x)$ and the fact that $y \\in \\{0,1\\}$, it's worth noting that $h_{\\theta}(x)=0.5$ is the decision boundary.\n",
      "\n",
      "This is produced when $\\theta^Tx = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 = 0$.\n",
      "\n",
      "In turn, we can define a line of values for $\\theta_{1,2}$ over which the decision is ambiguous.\n",
      "\n",
      "$x_2 = -\\frac{\\theta_1}{\\theta_2} x_1 + \\theta_3 $"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value for theta\n",
      "theta=np.array([4,-4,5],dtype=float)\n",
      "\n",
      "# Evaluate logistic funtion within the parameter space\n",
      "plt.subplot(2,2,1)\n",
      "from matplotlib.colors import ListedColormap\n",
      "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
      "# Create a granular grid of points\n",
      "x_min, x_max = 0,10\n",
      "y_min, y_max = 0,10\n",
      "h = .02  # step size in the mesh\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
      "# Predict the class for each\n",
      "Z = 1/(1+np.exp( -(theta[0]+xx.ravel()*theta[1]+yy.ravel()*theta[2]) ) )\n",
      "# Assign a color to each point in the mesh\n",
      "Z = Z.reshape(xx.shape)\n",
      "# Plot decision boundary\n",
      "plt.title('Evaluate hypothesis',fontsize=10)\n",
      "plt.pcolormesh(xx, yy, Z, cmap=\"hot\")\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "# Decision boundary\n",
      "plt.subplot(2,2,2)\n",
      "x=np.arange(x_min, x_max, .1 )\n",
      "y=-(1/theta[2])*(theta[1]*x+theta[0])\n",
      "plt.plot(x,y,lw=2,color='black',alpha=0.75)\n",
      "plt.xlim([0,10])\n",
      "plt.ylim([0,10])\n",
      "plt.title('Decision boundary',fontsize=10)\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAClCAYAAADmmQKUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x114daf410>"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, we train our model in order to find $\\theta_{1,2,n}$ that defines a decicion boundary that seperates our data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Maximum likihood model applied to logistic regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compress the two equations above: \n",
      "\n",
      "$$ L(\\theta) = P(y \\mid x;\\theta) = (h_{\\theta}(x))^y (1 - h_{\\theta}(x))^{1-y} $$ \n",
      "\n",
      "$$ l(\\theta) = log L(\\theta) = \\sum\\limits_{i=1}^m y^i \\times log h_{\\theta}(x^i) + (1-y^i) \\times log(1- h_{\\theta}(x^i)) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We previously showed that the maximum liklihood estimate for linear regression is used to derive the least squares cost funtion. \n",
      "\n",
      "As with $J(\\theta)$ for linear regression, $l(\\theta)$ for logistic regression is convex.\n",
      "\n",
      "As with $J(\\theta)$ minimization for linear regression, we now maximizes the liklihood, $l(\\theta)$.\n",
      "\n",
      "Based upon the class notes, we know the gradient ascent rule is used iterativly update $\\theta$: \n",
      "\n",
      "$ \\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $ where the update is simultanously performed for all values of $j$.\n",
      "\n",
      "This solves to:\n",
      "\n",
      "$$ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $$\n",
      "\n",
      "Thus, we know the following for a single training example:\n",
      "\n",
      "The $\\theta$ that maximizes this funtion will have $\\frac{\\partial}{\\partial \\theta_j} l(\\theta) = 0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradient ascent algorithm can be used to find $\\theta$ that maximizes $L(\\theta)$ - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is analogus to derivation of gradient descent in the case of linear regression.\n",
      "\n",
      "Previously we wanted to minimize our cost funtion, $J(\\theta)$. Now we want to maximize $l(\\theta)$.\n",
      "\n",
      "We use $\\textbf{gradient ascent}$ to iterativly update $\\theta$: $ \\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $.\n",
      "\n",
      "Solving this, as we did before, gives the stochastic gradient ascent updater rule:\n",
      "\n",
      "$$ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $$\n",
      "\n",
      "Recall for linear regression: $h_{\\theta}=\\theta^Tx$.\n",
      "\n",
      "Now, for logistic regression: $h_{\\theta}=\\frac{1}{1+e^{-\\theta^T x}}$.\n",
      "\n",
      "Generally, we will find a broad class of algorithms with the same form but different $h_{\\theta}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Another algorithm, Newton's method, for fitting logistic regression model - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For linear regression, we used the normal equations to find a closed-form solution for $\\theta$ that maximizes the liklihood.\n",
      "\n",
      "There is no analogous closed-form solution for the case of logistic regression.\n",
      "\n",
      "Again, to maximize or minimize a funtion we simply take the derivative and set it to zero.\n",
      "\n",
      "$ f(\\theta) = \\frac{d}{d\\theta}l(\\theta) $ and want to find $f(\\theta) = 0$.\n",
      "\n",
      "We know that $f(\\theta)$ is a curve and simply want to move along it for different values of $\\theta$.\n",
      "\n",
      "For any value of $\\theta_o$, $f'(\\theta_o) = \\frac{f(\\theta)'}{\\Delta}$ where $\\Delta$ is the distance between guesses for $\\theta$. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Perceptron learning algorithm - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With logistic regression, we had:\n",
      "\n",
      "$$ h_{\\theta}(x) = g(\\theta^T x) $$\n",
      "\n",
      "Recall that logistic regression had a probablistic interpretation:\n",
      "\n",
      "$$ P(y=1 \\mid x;\\theta) = h_{\\theta}(x) = g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^T x}}$$\n",
      "\n",
      "$$ P(y=0 \\mid x;\\theta) = 1 - h_{\\theta}(x) $$ \n",
      "\n",
      "Thus, we could define the likelihood function, which is convex.\n",
      "\n",
      "Then, we could maximize it using the gradient ascent rule.\n",
      "\n",
      "Now, for the Perceptron, we have defined a new function $g(z)$ that returns a step function:\n",
      "\n",
      "* 1 if $z$ $\\geq$ 0 \n",
      "* 0 if $z$ < 0\n",
      "\n",
      "We use the same updater rule for $\\theta$:\n",
      "    \n",
      "$$ \\theta_j := \\theta_j + \\alpha (y^i - h_{\\theta} (x^i)) x_j^i $$\n",
      "\n",
      "$$ \\theta_j := \\theta_j + \\alpha y^i x_j^i - \\alpha h_{\\theta} (x^i) x_j^i $$\n",
      "\n",
      "Let $y \\in {-1,1}$.\n",
      "\n",
      "If we guess correctly, then $y^i=-1$ and $h_{\\theta} (x^i)=-1$:\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha x_j^i + \\alpha x_j^i $$\n",
      "\n",
      "Thus, we make no update:\n",
      "\n",
      "If we guess incorrectly and $y^i=-1$ but $h_{\\theta}=1$:\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha x_j^i - \\alpha x_j^i $$\n",
      "\n",
      "$$ \\theta_j := \\theta_j - 2 \\alpha x_j^i $$\n",
      "\n",
      "Critically, it is difficult to:\n",
      "\n",
      "* Endow this prediction with probablistic interpretations.\n",
      "* Derive the perceptron as a maximum likelihood estimation algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generalized linear models - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Key concepts - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Exponential family\n",
      "* $ h(x) = E[y \\mid x] $\n",
      "* $ \\eta = \\theta^T x $"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Overview - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For both regression and classification problems, the output can be modeled assuming a particular probability distribution.\n",
      "\n",
      "For classification problems with binary output:\n",
      "\n",
      "* The output can be modeled as Bernoulli random variable, $ y \\mid x;\\theta $ ~ Bernoulli($\\phi$).\n",
      "* A Bernoulli random variable takes value 1 with success probability $\\phi$. $P(y=1) = \\phi $\n",
      "* For classification, the model parameter is re-cast as a hypothesis funtion: $ \\phi = h_{\\theta}(x) $ \n",
      "* For any training example, $ P(y^i \\mid x^i;\\theta) = h_{\\theta}(x)^{y^i} (1-h_{\\theta}(x^i))^{1-y^i} $\n",
      "\n",
      "For linear regression problems:\n",
      "\n",
      "* We assume inputs, $x^i$, are related to outputs, $y^i$, by $y^i = \\theta^T x^i + \\epsilon^i$. \n",
      "* The error term, $\\epsilon$, captures un-modeled effects and are independently and identically distributed according to a a Gaussian distribution.\n",
      "* $ p(\\epsilon) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(\\epsilon^i)^2 }{2 \\sigma^2} $\n",
      "* By substitution, we obtain: $ p(y^i \\mid x^i;\\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(y^i - h_{\\theta}(x^i))^2 }{2 \\sigma^2} $\n",
      "\n",
      "In both cases, we use data to train the distribution model parameters, $\\theta$.\n",
      "\n",
      "Specifically, we determine the value of $\\theta$ that maximizes the likelihood of observing our training data: \n",
      "\n",
      "* $ L(\\theta) = p(Y \\mid X;\\theta) = \\prod\\limits_{i=1}^m P(y^i \\mid x^i;\\theta) $\n",
      "* We compute the log likelihood by simply substituting our equation for $ p(y^i \\mid x^i;\\theta)$ into the above and taking the log.\n",
      "\n",
      "Maximizing the log likelihood for linear regression:\n",
      "\n",
      "* Can be done iteratively, using gradient ascent.\n",
      "* Gives the same result as minimizing our least squares cost funtion for linear regression.\n",
      "* Thus, the least squares cost funtion is easily derived from assuming a Gaussian distributed noise term.\n",
      "\n",
      "Maximizing the log likelihood for logistic regression:\n",
      "\n",
      "* Can be done using gradient ascent: $ \\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $\n",
      "* Computing $ \\frac{\\partial}{\\partial \\theta_j} l(\\theta) $ showed that the update rule is identical to that of least mean squares, $ \\theta_j := \\theta_j + \\alpha (y^i - (h_{\\theta} (x^i)) x_j^i $ \n",
      "* However, $h_{\\theta}(x^i)$ is a non-linear funtion in this case.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Rationale for generalized linear models - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To step back:\n",
      "\n",
      "* Supervised learning defines a function, $h(x)$, that outputs a target (guess) given a set of inputs (features).\n",
      "* The target can be continuous (for regression problems) or a categorical label (for classification problems).\n",
      "* Linear regression models the target as a linear combination of parameters and features $\\theta^Tx$.\n",
      "* If we assume a Gaussian model error term, we have a probablistic model to explain our training data: $ p(y \\mid x;\\theta) $.\n",
      "* Similarly, logistic regression models the target using the Sigmoid funtion. \n",
      "* If the target is a Bernoulli variable, we again have a probablistic model to explain our training data $ p(y \\mid x;\\theta) $.\n",
      "* We then have a framework (likelihood maximization) to find parameters $\\theta$ that explain the observed training data.\n",
      "\n",
      "Now, consider cases in which the target values we want to predict are not rationally modeled using these:\n",
      "\n",
      "* The data are not categorical, meaning a Bernoulli random variable does not make sense.\n",
      "* The errors are not obviously Gaussian.\n",
      "* For example, customer arrivals (or page-views) are well-modeled using a Poisson distribution.\n",
      "\n",
      "For this, we'll want the ability to build a learning model from a larger set of distributions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Refresher on Binomial distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the probability of observing $n$ successes for $N$ trials given a probability $p$ for each without regard for order:\n",
      "\n",
      "* The probability of any sequence of $n$ successes and $N-n$ failures: $p^n (1-p)^{N-n}$\n",
      "* Binomal coefficient is used as there are several ways to obtain $n$ successes $ {N \\choose n} = \\frac{N!}{n!(N-n)!} $.\n",
      "* The expected value is simply $pN$.\n",
      "* This is useful for sequence mapping problems (e.g., N reads mapping to a genome, use n to evaluate uniformity of coverage assuming p expected reads per base).  \n",
      "\n",
      "Because this is a discreet distriubtion, the probability mass function is:\n",
      "\n",
      "$$ P(n) = {N \\choose n} p^n (1-p)^{N-n} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Refresher on Poisson distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Poissoin is simply the Binomal for a large number of trials, each of which have low probability:\n",
      "\n",
      "* The expected value is $\\lambda = pN$\n",
      "\n",
      "The probability mass funtion:\n",
      " \n",
      "$$ P(X=k) = \\lim _{N \\to \\infty} {N \\choose k} \\frac{\\lambda}{N}^k (1-\\frac{\\lambda}{N})^{N-k} = \\frac{\\lambda^k}{k!} e^{-\\lambda} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Refresher on Exponential distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The exponential distribution models the waiting time between events in a Poisson process:\n",
      "    \n",
      "* The PDF decays in proportion to $\\lambda$.\n",
      "* In turn, the waiting time between events decays as $\\lambda$ increaces.\n",
      "\n",
      "$$ P(x) = \\lambda e^{-\\lambda x} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Structure for generalized linear models - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a large number of exponential family distributions.\n",
      "\n",
      "They can be all be written in a general form generalized linear model:\n",
      "\n",
      "$$P(y ; \\eta) = b(y) exp ( \\eta^T T(y) - a(\\eta) )$$\n",
      "\n",
      "Parameters b,T,a define Bernoulli, Gaussian, or any exponential family distribution.\n",
      "\n",
      "The following three features are central:\n",
      "\n",
      "* The distribution of $y$ follows  some exponential family distribution with parameters $\\eta$. \n",
      "* We want the prediction output by our learned hypothesis to satisfy $ h_{\\theta}(x) = E[y \\mid x] $.\n",
      "* The natural parameter, $\\eta$, and inputs $x$ are related linearly: $\\eta=\\theta^Tx$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The hypothesis function for logistic regression is derived from a GLM assuming a Bernoulli distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From our work above, we define the probability of observing our training data assuming a Bernoulli distribution.\n",
      "\n",
      "$$ L(\\theta) = P(y \\mid x;\\theta) = (h_{\\theta}(x))^y (1 - h_{\\theta}(x))^{1-y} $$ \n",
      "    \n",
      "To simplify, we can state:\n",
      "    \n",
      "$$P(y;\\phi) = \\phi^y (1-\\phi)^{1-y}$$\n",
      "\n",
      "Recall that we always want to re-cast the equation in the following form to extract GLM terms:\n",
      "\n",
      "$$ P(y ; \\eta) = b(y) exp ( \\eta^T T(y) - a(\\eta) )$$\n",
      "\n",
      "Steps:\n",
      "\n",
      "* Raise the log to the power of e.\n",
      "* Re-arrange terms.\n",
      "\n",
      "$$ P(y ; \\eta) =  exp ( (log \\frac{\\phi}{1-\\phi}) y + log(1-\\phi) ) $$\n",
      "$$ \\eta = log\\frac{\\phi}{1-\\phi} $$\n",
      "$$ T(y) = y $$\n",
      "$$ a(\\eta) = log(1-\\phi)$$\n",
      "\n",
      "Inverting the first equation gives the logistic funtion:\n",
      "\n",
      "$$ \\phi = \\frac{1}{1 + e^{-\\eta}} $$\n",
      "\n",
      "Moreover, the logistic funtion sets the relationship between Bernoulli parameter $\\phi$ and generalized linear model parameter $\\eta$ to ensure equality, as defined above.\n",
      "\n",
      "Because $E[y \\mid x]=\\phi$ for a Bernoulli random variable:\n",
      "\n",
      "$$ h_{\\theta}(x) = E[y \\mid x] = \\phi = \\frac{1}{1 + e^{-\\eta}} = \\frac{1}{1 + e^{-x\\theta}} $$\n",
      "\n",
      "In turn, the logistic regression hypothesis funtion is derived by assuming a GLM for the target with Bernoulli distribution. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The hypothesis function for linear regression is derived from a GLM assuming a Normal distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the density function for Gaussian distribution:\n",
      "\n",
      "$$ P(y;\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(-\\frac{(y-\\mu)^{2}}{2\\sigma^2}) $$ \n",
      "\n",
      "With $\\sigma^2=1$ to simplify:\n",
      "\n",
      "$$ P(y;\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(-\\frac{1}{2} (y-\\mu)^2) $$ \n",
      "\n",
      "Steps:\n",
      "\n",
      "* Distribute $(y-\\mu)^2$.\n",
      "* Seperate terms.\n",
      "\n",
      "$$ P(y;\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(-0.5 y^2) exp(\\mu y - 0.5\\mu^2) $$\n",
      "\n",
      "We know that \n",
      "\n",
      "$$T(y)=y$$\n",
      "\n",
      "So, we can then easily see: \n",
      "\n",
      "$$\\eta=\\mu$$\n",
      "$$a(\\eta)=0.5\\mu^2$$\n",
      "$$b(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(-0.5 y^2) $$\n",
      "\n",
      "Like the case above, we know the expected value of the Gaussian:\n",
      "\n",
      "$$ E(y \\mid x;\\theta) = h_{\\theta}(x) = \\mu $$\n",
      "\n",
      "Thus, we determine the mapping between Gaussian paramter $\\theta$ and exponential paramter $\\eta$:\n",
      "\n",
      "$$ E(y \\mid x;\\theta) = h_{\\theta}(x) = \\mu = \\eta = \\theta^Tx$$\n",
      "\n",
      "Interestingly, the linear regression model falls out of a generalized linear model assuming Normal distribution for the output."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Derive a GLM assuming the Poisson distribution - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Knowing the probability mass funtion for a Poisson random variable:\n",
      "    \n",
      "$$ P(y; \\lambda) = \\frac{\\lambda^k}{k!} e^{-\\lambda} $$\n",
      "    \n",
      "Intutition:\n",
      "\n",
      "* We want to be able to write the distribution parameter as a funtion of the GLM parameter.\n",
      "* Thus, we can re-arrange terms.\n",
      "\n",
      "$$ P(k; \\lambda) = \\frac{1}{k!} e^{k log (\\lambda)} e^{-\\lambda} $$\n",
      "$$ P(y; \\lambda) = \\frac{1}{y!} exp (y \\times log (\\lambda) -\\lambda) $$\n",
      "\n",
      "We know that \n",
      "\n",
      "$$T(y)=y$$\n",
      "\n",
      "So, we can then easily see: \n",
      "\n",
      "$$\\eta=log (\\lambda)$$\n",
      "$$a(\\eta)=\\lambda = e^{\\eta}$$\n",
      "$$b(y) = \\frac{1}{y!} $$\n",
      "\n",
      "The GLM is:\n",
      "\n",
      "$$ P(y; \\lambda) = \\frac{1}{y!} exp ( y \\times \\eta - e^{\\eta} ) $$\n",
      "\n",
      "In turn, the response function for the GLM will be:\n",
      "\n",
      "$$ E(y \\mid \\eta) = E(y \\mid x;\\theta) = h_{\\theta}(x) = \\lambda = e^{\\eta}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using a GLM - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a GLM, how will we use the resulting model? \n",
      "\n",
      "Recall the objective:\n",
      "\n",
      "* The objective is to train our hypothesis parameters on a set of training data.\n",
      "* We have been doing this using maximum likelihood estimation, finding parameters for which the training data is most likely.\n",
      "* We can solve to $\\theta$ using gradient ascent.\n",
      "\n",
      "Recall the key insights derived from GLM:\n",
      "\n",
      "* We start with a probablistic model for the data, with $p(y^i \\mid x^i;\\theta)$.\n",
      "* For linear regression: $ p(y^i \\mid x^i;\\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} exp \\frac{ -(y^i - h_{\\theta}(x^i))^2 }{2 \\sigma^2} $\n",
      "* For logistic regression: $ P(y^i \\mid x^i;\\theta) = h_{\\theta}(x^i)^{y^i} (1-h_{\\theta}(x^i))^{1-y^i} $\n",
      "* GLM is used to derive the hypothesis funtion for each case, $h_{\\theta}$.\n",
      "\n",
      "With $p(y^i \\mid x^i;\\theta)$ and $h_{\\theta}$ defined, we can use MLE to find $\\theta$:\n",
      "\n",
      "* The log likelihood for an example is $l(\\theta) = log p(y^i \\mid x^i;\\theta) $\n",
      "* The gradient ascent update rule is: $ \\theta_j := \\theta_j + \\alpha \\frac{\\partial l(\\theta)}{\\partial \\theta} $\n",
      "* Knowing $p(y^i \\mid x^i;\\theta)$ and $h_{\\theta}$, solve  $\\frac{\\partial l(\\theta)}{\\partial \\theta_j}$ and plug it into the obove update rule. \n",
      "    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Justification for loglikelihood as a concave funtion with a single maximum, allowing use of the gradient descent algorithm - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The loglikelihood for a single example:\n",
      "    \n",
      "$$ l(\\theta) = log(p(y^i \\mid x^i;\\theta)) $$\n",
      "\n",
      "Given a training set, the loglikelihood is:\n",
      "    \n",
      "$$ l(\\theta) = \\sum_{i=1}^m log ( p(y^i \\mid x^i;\\theta) ) $$\n",
      "\n",
      "The approach to determining whether a funtion is concave is is compute the Hessian, H matrix, of the funtion and show:\n",
      "\n",
      "* The matrix H is negative semi-definite ($H \\leq 0$).\n",
      "* This implies that l is concave and has no local maxima other than the global one. \n",
      "\n",
      "Computing the Hessian requires:\n",
      "\n",
      "* Compute partial with respect to one variable, first: \\frac{\\partial h_{\\theta} (x) }\n",
      "* Usually this will have an straightforward solution.\n",
      "* Then, compute the partial with respect to the second variable.\n",
      "\n",
      "Justification for the logistic regression likelihood funtion :\n",
      "\n",
      "* From class notes: $ \\frac{\\partial l(\\theta)}{\\partial \\theta_j} = y - (h_{\\theta} (x)) x_j $\n",
      "* Then, $ \\frac{\\partial^2 l(\\theta)}{\\partial \\theta_j \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} [ y - (h_{\\theta} (x)) x_j ] = - \\frac{\\partial h_{\\theta} (x) } { \\partial \\theta_k } x_j  $ \n",
      "* We know $h_{\\theta} (x)$ is the sigmoid function.\n",
      "* Taking this derivative and applying chain rule to the exponent: $ H_{ij} = \\frac{\\partial h_{\\theta} (x) } { \\partial \\theta_k } = h_{\\theta} (x ) (1 - h_{\\theta} (x)) x_k x_j  $\n",
      "* $ zHz^T = \\sum\\limits_{i=1}^m \\sum\\limits_{i=1}^m z_i z_j \\times - h_{\\theta} (x ) (1 - h_{\\theta} (x)) x_k x_j $\n",
      "* We can pull out the constant term over the summation.\n",
      "* $ zHz^T = - h_{\\theta} (x ) (1 - h_{\\theta} (x)) \\sum\\limits_{i=1}^m \\sum\\limits_{i=1}^m z_i z_j x_k x_j $\n",
      "* $ h_{\\theta} (x ) (1 - h_{\\theta} (x)) \\geq 0 $ and $ \\sum\\limits_i \\sum\\limits_j z_i x_i z_j x_j \\geq 0 $\n",
      "* Finally, $ zHz^T \\leq 0 $"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Softmax regression - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A type of classification problem where y can take on $k$ different classes where $y \\in \\{0,1, ... k\\}$.\n",
      "\n",
      "Parameters of the multinomial specify the liklihood of getting each outcome, and all add to one.\n",
      "\n",
      "We need to find b, T, a such that any multinomial distrubtion can be re-cast as a generalized linear model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generative learning algorithms - "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Key concepts - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Bayes rule\n",
      "* GDA (decision boundary)\n",
      "* Naive Bayes and Laplace smoothing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Intro - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The prior learning algorithms that we discussed (linear and logistic regression) were $\\textbf{discriminatative}$.\n",
      "\n",
      "It will try to learn the output given the features: $P(y \\mid x)$.\n",
      "\n",
      "Alternativly, we can focus just on building a model for each of the classes indepedently.\n",
      "\n",
      "In these cases, we try to model the distribution of features for either class.\n",
      "\n",
      "Then, we can try to match new samples to both models.\n",
      "\n",
      "These $\\textbf{generative}$ models learn the features, given the label, $P(x \\mid y)$, and the class prior $P(y)$.\n",
      "\n",
      "We can then use $\\textbf{Bayes rule}$: \n",
      "\n",
      "$$ P(Y=1|X) = \\frac{P(X|y=1)P(y=1)}{P(X)} $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gaussain discriminant analysis -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with all generative learning algorithms, we learn the probability of observing a set of feature for each class label $P(X|y)$. \n",
      "\n",
      "Let's start with a simple and fast, but inaccurate, learning algorithm.\n",
      "\n",
      "We simply establish a probability density function of features for each class label:\n",
      "\n",
      "* We assume features can be modeled using a normal distribution.\n",
      "* We use the multivariate Gaussian to model the probability of feature values with respect to class type.\n",
      "\n",
      "$$ P(x \\mid y_o) = \\frac{1}{\\Sigma^{0.5} (2 \\pi)^{0.5n} } exp -\\frac{ (x-\\mu_o) (x-\\mu_o)^T }{2 \\Sigma} $$\n",
      "\n",
      "Where:\n",
      "\n",
      "* The class labels are either 0 or 1: $ P(y) = \\phi^y (1-\\phi)^{1-y} $ \n",
      "* Model $P(X|y=0)$ ia the multi-variate Gaussian parameterized by $\\mu_o$.\n",
      "* Model $P(X|y=0)$ ia the multi-variate Gaussian parameterized by $\\mu_1$.\n",
      "* Both models share a covariance matrix and, thus, have the same shape but are centered around different mean values.\n",
      "\n",
      "Our strategy is as follows:\n",
      "\n",
      "* We define a joint loglikelihood function: $l(\\phi,\\mu_o,\\mu_1,\\Sigma) = \\sum_{i=1}^m log (P(x \\mid y)) p(y)$.\n",
      "* Maximizing the loglikelihood is used to derive formulas for the maximum likelihood estimates of each parameter.\n",
      "* In turn, we use the training data to compute the maximum likelihood estimates of each parameter.\n",
      "* With new data, we can simply compute the posterior probability for each class and pick the better one.\n",
      "\n",
      "For a given training example, the posterior probability $P(y \\mid x)$ is computed as follows: $P(y \\mid x) = P(x \\mid y) P(y)$:\n",
      "        \n",
      "* $P(y) = \\phi$\n",
      "* $P(x \\mid y)$ is our model, the multivariate Gaussian.\n",
      "\n",
      "Note that this implies that the decision boundary will be found at:\n",
      "\n",
      "* $P(y=0 \\mid x) = P(y=1 \\mid x)$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall how we derive the maximum likelihood formulas for each parameter.\n",
      "\n",
      "We take the derivative of the likelihood function with respect to each and set equal to zero:\n",
      "\n",
      "* $\\frac{\\partial l}{\\partial \\theta}$\n",
      "* $\\nabla_{\\mu_o} l$ and $\\nabla_{\\mu_1} l$\n",
      "* $\\nabla_{S} l$ where $S=\\frac{1}{\\Sigma}$\n",
      "\n",
      "The derivations are fairly involoved (HW#1), but the intution is simply that we use them to compute our model parameters from the training data! "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An interesting point about GDA is that the posterior probability can be simplified into a form that looks like logistic regression:\n",
      "\n",
      "* Starting with $P(y \\mid x) = P(x \\mid y) P(y)$.\n",
      "* Perform substituions and simplifications.\n",
      "\n",
      "$$P(y \\mid x) = \\frac{1}{1+exp(-\\theta x)} $$\n",
      "\n",
      "Where $\\theta = f(\\mu_o,\\mu_1,\\Sigma,\\phi)$ \n",
      "\n",
      "GDA is a better algorithm than logistic regression when:\n",
      "\n",
      "* The class features can be modeled as Gaussian.\n",
      "* $\\Sigma$ is shared between classes.\n",
      "\n",
      "However, logistic regression is more robust, as it makes fewer modeling assumptions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Naive bayes -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, our prior approach was to use a multinomial distribution to model the probability of observing different combinations of features given the class label.\n",
      "\n",
      "Now, consider problems like text classification in which the feature space (e.g., a large dictionary of words) is huge.\n",
      "\n",
      "Given a class label (e.g., spam), it would be challenging to model all combinations using multinomial distribution.\n",
      "\n",
      "For text classification, we convert an e-mail into a bit vector where each position specifies a word out of a (large, e.g., 10,000 word) dictionary:\n",
      "\n",
      "Critically, the Naive Bayes assumption is that each word, $x_i$, is conditionally independent, given y:\n",
      "\n",
      "$P(X \\mid y) = P(x_1,x_2,x_3 \\mid y) = P(x_1 \\mid y)$$P(x_2 \\mid y)$$P(x_3 \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y_i)$\n",
      "\n",
      "When m indexes over the dictionary and the probabiluty of each word appearing is independent of seeing any other.\n",
      "\n",
      "Using Bayes rule, we then compute the prected class for any given e-mail:\n",
      "\n",
      "$$ P(Y=1\\mid X) = \\frac{P(X \\mid y=1)P(y=1)}{P(X)} $$\n",
      "\n",
      "Furthermore, in class we also showed that we can compute the joint likelihood, which is the product of predicted value for our class. \n",
      "\n",
      "We can take the derivitive, set it to zero (to find the maximum), and then find the expected parameters.\n",
      "\n",
      "$$ l() = \\sum_{i=1}^m log (P(x \\mid y)) p(y)$$\n",
      "\n",
      "This results in very intuitive estimates for the model parameters:\n",
      "\n",
      "For example $P(j \\mid y=0)$ is simply the ocourance of the given word divided by the total number of e-mails in the class $y=0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Limitations with Naive Bayes and Laplace smoothing - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For events that we have not yet seen, $P(X=j \\mid y)=0$ based upon our expected value formula for $P(j \\mid y)$.\n",
      "\n",
      "Thus, $P(X \\mid y) = \\prod\\limits_{i=1}^{m} p(x_i \\mid y) = 0$ and the classifier crashes.\n",
      "\n",
      "Laplace smoothing, initially used to compute the likelihood of the sun rising, simply adds one to address the zero problem."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Non-linear decision bounaries and SVMs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Refresher on convex optimization - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convex optimization: \n",
      "    \n",
      "* Find a global solution for objective function within polynomial time.\n",
      "\n",
      "Convex sets:\n",
      "\n",
      "* Think of the set as a circle.\n",
      "* We draw a line between any two points.\n",
      "* If all points on the line are within the set, the set ($C$) is convex.\n",
      "* All points on a straight line that passes between $x_1,x_2$ are described by: $ x_3 = \\theta x_1 + (1-\\theta) x_2 \\in C $.\n",
      "* If this statement is true for any $x,y$, then the set is convex.\n",
      "* Examples include: All real numbers, non-negative numbers, norm ball ($\\| 1 \\| \\leq 1$). \n",
      "* Importaintly, positive semi-definite matricies are a convex set.\n",
      "\n",
      "Convex functions:\n",
      "\n",
      "* Convex funtion: $ f(\\theta x_1 + (1-\\theta) x_2) \\leq f(x_1) + (1-\\theta)f(x_2) $\n",
      "* Will always have a global optimum.\n",
      "\n",
      "First order condition of convexity: \n",
      "\n",
      "* The intution here is that we show that a function looks like a quadratic.\n",
      "* $f(x_2) \\geq f(x_1) +\\nabla f(x_1)^T (x_2-x_1)$\n",
      "* Simply, $x_2-x_1$ is the $x$ distance between points.\n",
      "* $\\nabla f(x_1)^T$ is the gradient of the function at $x_1$.\n",
      "* $\\nabla f(x_1)^T (x_2-x_1)$ gives an estimate for $f(x_2)$.\n",
      "* We simply show that the actual value of $f(x_2)$ is always greater than this estimate.\n",
      "\n",
      "Second order condition of convexity: \n",
      "\n",
      "* The second derivative is $\\geq 0$.\n",
      "* For example, proove that $y=x^2$ is convex: the second derivative is 2, which is $\\geq 0$.\n",
      "* Note that this can easily be extended to more complex functions: if working with a matrix, simply take the Hessian.\n",
      "* Moreover, if Hessian matrix is positive semi-definite, then the function is convex.\n",
      "\n",
      "The convex set and function come together to solve problems with a specific form:\n",
      "\n",
      "* Minimize a convex function, $f(x)$.\n",
      "* Subject to $x \\in C$.\n",
      "* Also subject to inequality ($g_i(x) \\leq 0$) and equality ($h_i=0$) constraints.\n",
      "* Where $f(x)$ and $g_i(x)$ are both convex differentiable.\n",
      "* Where $h_i(x)$ is affine, meaning that it is both convex and concave. \n",
      "\n",
      "Special cases:\n",
      "\n",
      "* Linear programming: Objective is affine, inequality condition is affine.\n",
      "* Quadratic programming: Objective is quadratic (e.g., least squares cost function)\n",
      "\n",
      "Use of quadratic programming for least squares minimization:\n",
      "\n",
      "* Typical problem will be constrained least squares.\n",
      "* We want to minimize our model error.\n",
      "* Objective is $\\frac{1}{2} \\| Ax - b \\|^2$\n",
      "* With constraints: (1) matrix format, (2) equality and / or inequality constraints."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear decision boundaries - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we showed that logistic regression is a classification algorithm. \n",
      "\n",
      "Given $h_{\\theta}(x)$ and the fact that $y \\in \\{0,1\\}$, it's worth noting that $h_{\\theta}(x)=0.5$ is the decision boundary.\n",
      "\n",
      "This is produced when $\\theta^Tx = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 = 0$.\n",
      "\n",
      "In turn, we can define a line of values for $\\theta_{1,2}$ over which the decision is ambiguous.\n",
      "\n",
      "In this case, the decision boundary is linear:\n",
      "\n",
      "$x_2 = -\\frac{\\theta_1}{\\theta_2} x_1 + \\theta_3 $"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value for theta\n",
      "theta=np.array([4,-4,5],dtype=float)\n",
      "\n",
      "# Decision boundary\n",
      "plt.subplot(2,2,2)\n",
      "x_min=0\n",
      "x_max=10\n",
      "x=np.arange(x_min, x_max, .1 )\n",
      "y=-(1/theta[2])*(theta[1]*x+theta[0])\n",
      "plt.plot(x,y,lw=2,color='black',alpha=0.75)\n",
      "plt.xlim([0,10])\n",
      "plt.ylim([0,10])\n",
      "plt.title('Decision boundary',fontsize=10)\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAOgAAAClCAYAAABWb7HKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFULH/h/IJO73oN0DFQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1098a4e50>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, we train our model to obtain parameter values ($\\theta_{1,2,n}$) that seperate our data.\n",
      " \n",
      "Yet, in some cases, a linear decision boundary will not nicely seperate our data.\n",
      "\n",
      "Thus, we need to define a classification algorithm that implement decision boundaries with different shapes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Funtional and geometric margin -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume our training set is linearly seperable.\n",
      "\n",
      "$g(z)$ is the logistic funtion where $z=\\theta^T x$:\n",
      "    \n",
      "* If $y^i=1$, we train to generate $\\theta^T x >> 0$ and our classifier picks 1.\n",
      "* If $y^i=0$, we train to generate $\\theta^T x << 0$ and our classifier picks 0.\n",
      "\n",
      "The concept of $\\textbf{funtional margin}$ simply tells us whether a given training example is properly classified.\n",
      "\n",
      "Given a training dataset that is linealy seperable, some bounaries come closer to training examples (less favorable) than others.\n",
      "\n",
      "Moreover, we might choose the decision bounary that stays far away from our training examples.\n",
      "\n",
      "The concept of $\\textbf{geometric margin}$ tells us how close the decision boundary is to each training example.\n",
      "\n",
      "Useful review:\n",
      "\n",
      "* http://stackoverflow.com/questions/20058036/svm-what-is-a-functional-margin"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The functional margin for the SVM classifier indicates whether training examples are properly classified -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For logistic regression:\n",
      "\n",
      "* Classes: $y \\in \\{0,1\\}$.\n",
      "* Hypothesis: $h(z) = g(z)$ where $g(z)$ is the logistic funtion.\n",
      "* Input: $z=\\theta^T x$ where a linear combination of features and parameters is used.\n",
      "* Funtional margin: if $y^i=1$, we train to generate $\\theta^T x >> 0$ and our classifier picks 1.\n",
      "\n",
      "For a new classifier that sets the stage for SVMs:\n",
      "\n",
      "* Classes: $y \\in \\{-1,1\\}$.\n",
      "* Hypothesis: $h(z) = g(z)$ where $g(z)$ is a yet undefined step-funtion.\n",
      "* Input: $z=w^Tx + b$, again a linear combination of features and parameters is used.\n",
      "* Funtional margin: if $y^i=1$, we train to generate $w^Tx + b >> 0$ and our classifier picks 1.\n",
      "\n",
      "The funtional margin with respect to a single training example:\n",
      "\n",
      "$\\hat{\\gamma}_i = y^i (w^T x^i + b)$\n",
      "\n",
      "The funtional margin with respect to a full training set is defined as the worst-case functonal margin:\n",
      "\n",
      "$\\hat{\\gamma} = min (\\hat{\\gamma}^i)$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Decision boundary -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with logistic regression, the decision boundary is produced when $\\omega^Tx + b = \\omega_1 x_1 + \\omega_2 x_2 + b = 0$.\n",
      "\n",
      "The linear decision boundary is:\n",
      "\n",
      "$x_2 = -\\frac{\\omega_1}{\\omega_2} x_1 - \\frac{b}{\\omega_2} $\n",
      "\n",
      "Note that the slope of the decision boundary is $-\\frac{\\omega_1}{\\omega_2}$, which is orthogomal to the slope defined by the parameters, $w$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value for theta\n",
      "w=np.array([4,4],dtype=float)\n",
      "b=5\n",
      "\n",
      "# Vals\n",
      "x_min=-10\n",
      "x_max=10\n",
      "\n",
      "# Ensure equal figure dimentions to see Orthogonality\n",
      "fig=plt.figure(figsize=(5,5))\n",
      "\n",
      "# Decision boundary\n",
      "plt.subplot(2,2,2)\n",
      "x=np.arange(x_min, x_max, .1 )\n",
      "y=-w[0]/w[1]*x-b/w[1]\n",
      "plt.plot(x,y,lw=2,color='black',alpha=0.75)\n",
      "plt.xlim([x_min,x_max])\n",
      "plt.ylim([x_min,x_max])\n",
      "plt.title('$\\omega$ and the decision boundary',fontsize=10)\n",
      "plt.xlabel('$x_1$',fontsize=10)\n",
      "plt.ylabel('$x_2$',fontsize=10)\n",
      "\n",
      "y_o=w[1]/w[0]*x\n",
      "plt.plot(x,y_o,lw=2,color='red',alpha=0.75)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAMcAAADKCAYAAAARm1QYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc0gkOpDmkEh0IM0hkehAmkMi0cH/A+aLJq1AoyYKAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1099af710>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that $\\omega$ (red) is orthogonal to the decision boundary:\n",
      "\n",
      "* We define the geometric margin, $\\gamma_i$, as the distance between a point and the boundary.\n",
      "* We also introduce the norm of $\\omega$: $\\sqrt(\\sum \\omega_i^2)$.\n",
      "* A unit vector in the direction of $\\omega$: $\\frac{\\omega}{\\|\\omega\\|}$\n",
      "\n",
      "The geometric margin for any point, $x^i$, is derived knowing that $x^i - \\gamma_i \\frac{\\omega}{\\|\\omega\\|}$ is on the decision boundary.\n",
      "\n",
      "Any point on the decision boundary evaluates to zero: \n",
      "\n",
      "$$ w^T (x^i - \\gamma_i \\frac{\\omega}{\\|\\omega\\|}) + b $$\n",
      "\n",
      "Moreover, you can solve for $\\gamma_i$:\n",
      "\n",
      "$$ \\gamma_i = y^i ((\\frac{\\omega}{\\|\\omega\\|})^T x^i + \\frac{b}{\\|\\omega\\|}) $$\n",
      "\n",
      "Recall that the functional margin is:\n",
      "\n",
      "$\\hat{\\gamma}_i = y^i (\\omega^T x^i + b)$\n",
      "\n",
      "Thus the two margins are equal for model parameters with $\\|w\\|=1$.\n",
      "\n",
      "$\\gamma_i = \\frac{\\hat{\\gamma_i}}{\\|w\\|}$\n",
      "\n",
      "And, finally, the geometric margin for a set of training data is the minimum computed for the training set:\n",
      "\n",
      "$\\gamma = min (\\gamma_i) $"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The geometric margin for the SVM classifier indicates how close training data are to the decision boundary -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, we want to find parameters ($\\omega,b$) that maximize the geometric margin for a training set:\n",
      "\n",
      "$max (\\frac{\\hat{\\gamma}}{\\|\\omega\\|})$\n",
      "\n",
      "The challenge is that we have a non-convex objective.\n",
      "\n",
      "We do several things to simply this:\n",
      "\n",
      "* Enforce that the funtional margin, $\\hat{\\gamma} = 1 $.\n",
      "* Thus, the geometric margin is $\\gamma = \\frac{1}{\\|w\\|}$.\n",
      "* Observe that maximizing $\\frac{1}{\\|w\\|}$ is the same as minimizing $\\frac{1}{2}(\\|w\\|)^2$\n",
      "\n",
      "We now want to find $w$ and $b$ such that:\n",
      "\n",
      "$max (\\frac{1}{\\|w\\|})$ or $min_{\\gamma,w,b} (\\|w\\|^{2})$, which is a quadratic constraint ($w^Tw$).\n",
      "\n",
      "Solving this problem (e.g., using QP solvers) gives us the optimal margin classifier!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The Lagrangian is a useful tool for solving optimization problems - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a convex function that we want to minimize with inequality and equality constraints:\n",
      "\n",
      "* $min_x$ $f(x)$ \n",
      "* $g_i(w) \\leq 0$ \n",
      "* $h_i(w) = 0$\n",
      "\n",
      "We can solve it using the Lagrangian, which is passed x (the primal variable) as well as $\\alpha$ and $\\beta$ where $L$ is convex in x:\n",
      "\n",
      "$$ L(x,\\alpha,\\beta) = f(x) + \\sum\\limits_{i=1}^m \\alpha_i g_i(x) + \\sum\\limits_{i=1}^m \\beta_i h_i(x) $$\n",
      "\n",
      "Importaintly:\n",
      "\n",
      "* The first term captures the inequality constrains: $g_i(w) \\leq 0$\n",
      "* The second term captures the equality constraints: $h_i(w) = 0$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The primal problem -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall our objective is $min_x$ $f(x)$.\n",
      "\n",
      "In the primal problem, we first maximize the Lagrangian function over all possible values of $\\alpha$ and $\\beta$.\n",
      "\n",
      "$$ \\theta_p(x) = max_{\\alpha, \\beta, \\alpha \\geq 0} L(x,\\alpha,\\beta) = f(x) + max_{\\alpha, \\beta, \\alpha \\geq 0} [ ... ] $$\n",
      "\n",
      "This is subject to the constraint that $\\alpha_i \\geq 0$.\n",
      "\n",
      "The problem is primal feasible if the initial constraints are satisfied: \n",
      "\n",
      "* $h_i=0$ \n",
      "* $g_i \\leq 0$.\n",
      "\n",
      "This is because:\n",
      "\n",
      "* The second summation drops due to $h_i=0$.\n",
      "* The first summation drops: $\\alpha_i = 0$) will maximize the funtion by knocking out the negative terms ($g_i \\leq 0$).\n",
      "\n",
      "Therefore, for values of $x$ that are primal feasible, the objective is:\n",
      "\n",
      "* $ min_x \\theta_p(x) = f(x) $\n",
      "* $P^* = min_x \\theta_p(x)$ is the optimal value of the objective.\n",
      "\n",
      "If constraints are violated (e.g., $g_i \\geq 0$):\n",
      "\n",
      "* Then the summation blows up and $ \\theta_p(x) = \\infty $."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The dual problem -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, recall our objective is $min_x$ $f(x)$.\n",
      "\n",
      "Another way of solving this is the dual problem, which reverses the order of operations.\n",
      "\n",
      "Previously we:\n",
      "\n",
      "* We maximized the Lagrangian function over all possible values of $\\alpha$ and $\\beta$.\n",
      "* For all values of $x$ that were primal feasible, we then got a simple objective that we then minimized over $x$.\n",
      "\n",
      "For the dual problem, we:\n",
      "\n",
      "* We minimize the Lagrangian function over all possible values of $x$.\n",
      "* Then, we get an objective that we maximize over all possible values of $\\alpha$ and $\\beta$!\n",
      "\n",
      "Thus, the dual objective, which is dual feasible if $\\alpha_i \\geq 0$.\n",
      "\n",
      "$$ \\theta(\\alpha,\\beta) = min_{x} L(x,\\alpha,\\beta) $$\n",
      "\n",
      "$$ D^* = max_{\\alpha,\\beta} \\theta(\\alpha,\\beta)$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Connecting the dual and primal problems - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Key points connecting the primal and dual problems:\n",
      "\n",
      "* Weak duality: The optimal value for the dual problem, d^*,  is $\\leq$ than that of the primal problem p^*. \n",
      "* And strong duality: $d^* = p^*$\n",
      "\n",
      "Therefore:\n",
      "\n",
      "* The role of the Lagrangian is to connect the primal and dual problems.\n",
      "* There are a set of parameters that solve both problems."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Karush-Kuhn-Tucker (KKT) conditions are parameters that simultanously solve the primal and dual problem - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These set of parameters that solve both dual and primal framing of the problem:\n",
      "\n",
      "* There is a value $\\omega^*$ that is a solution to the primal problem \n",
      "* There are values $\\alpha^*, \\beta^*$ that are solutions to the dual problem.\n",
      "\n",
      "These parameters therefore satisfy the KKT conditions, the most importaint of which is the dual complementarity condition:\n",
      "\n",
      "* $\\alpha^* g_i(\\omega^*) = 0$\n",
      "* $\\alpha^* \\geq 0$\n",
      "\n",
      "This implies that $\\alpha^* > 0$ only when $g_i(\\omega^*) = 0$.\n",
      "\n",
      "In the context of SVMs, recall that $g_i$ will be:\n",
      "\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $\n",
      "* $g_i = 0$ for points with a funtional margin of 1.\n",
      "* Points with a funtional margin of 1 are definitionally closest to the decision boundary."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Set up and solve the Dual problem to find optimal parameter values $w,\\alpha$ that maximize the geometric margin -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Previously, we defined a generalized Lagrangian:\n",
      "    \n",
      "$$ L(x,\\alpha,\\beta) = f(x) + \\sum\\limits_{i=1}^m \\alpha_i g_i(x) + \\sum\\limits_{i=1}^m \\beta_i h_i(x) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use this to maximize the geometric margin for the optimal margin classifier:\n",
      "\n",
      "* $f(w) = \\|w\\|^{2}$\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $\n",
      "\n",
      "Perform the substitution, noting that there are no equality ($h_i$) constraints in our problem:\n",
      "\n",
      "$$ L(w,b,\\alpha) = \\|w\\|^{2} - \\sum\\limits_{i = 1}^{k} \\alpha_i [y^i (w^T x^i + b) -1] $$\n",
      "\n",
      "We proceed to find the dual form of this problem, which has the classic form:\n",
      "\n",
      "$ \\theta(\\alpha,\\beta) = min_{x} L(x,\\alpha,\\beta) $\n",
      "\n",
      "We re-write with $x=\\omega+b$ and also note that $\\beta$ term is not present:\n",
      "\n",
      "$ \\theta(\\alpha,\\beta) = min_{\\omega,b} L(\\omega,b,\\alpha) $\n",
      "\n",
      "We can find the minimum by setting the derivative of $\\theta(\\alpha,\\beta)$ to zero:\n",
      "\n",
      "$$ \\nabla L(\\omega,b,\\alpha) = \\omega  - \\sum_{i=1}^m \\alpha_i y^i x^i = 0 $$\n",
      "\n",
      "We do this and substitue back into the inital Lagrangian to get:\n",
      "\n",
      "$$ min_w [ L(w,b,\\alpha) ] = \\sum \\alpha_i - 0.5 \\sum_i \\sum_j y^i y^j \\alpha_i \\alpha_j - <x^i , x^j > - b \\sum \\alpha_i y^i $$\n",
      "\n",
      "Critically, our problem meets conditions required for $p^* = d^*$ and the KKT conditions.\n",
      "\n",
      "This implies that $\\alpha^* > 0$ only when $g_i(\\omega^*) = 0$, the points close to our boundary.\n",
      "\n",
      "Moreover, we have re-framed the optimization problem:\n",
      "\n",
      "$$ max_{\\alpha; \\alpha \\geq 0} W(\\alpha) = \\sum \\alpha_i - 0.5 \\sum_i \\sum_j y^i y^j \\alpha_i \\alpha_j - <x^i , x^j > - b \\sum \\alpha_i y^i $$\n",
      "\n",
      "By solving the dual problem, we found: \n",
      "\n",
      "* A set of optimal $\\alpha_i$ values.\n",
      "* From KKT, we know that the $\\alpha_i>0$ only for points closest to the decision boundary, the support vector.\n",
      "* The optimal value of our model parameters: $ \\omega  = \\sum_{i=1}^m \\alpha_i y^i x^i $\n",
      "\n",
      "Now, if we want to make a prediction for a new point, x: \n",
      "\n",
      "$$ \\omega^T x + b = (\\sum_{i=1}^m \\alpha_i y^i x^i)^T x + b = \\sum_{i=1}^m \\alpha_i y^i <x^i x> + b $$\n",
      "\n",
      "Note two critical points:\n",
      "\n",
      "* This can be Kernelized, because only inner products between new point and the support vectors appear. \n",
      "* The only values of $\\alpha > 0$ are those for the support vectors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Apply the Dual problem to a different objective - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the general form of our constraints for the primal problem:\n",
      "    \n",
      "* $min_x$ $f(x)$ \n",
      "* $g_i(w) \\leq 0$ \n",
      "* $h_i(w) = 0$\n",
      "    \n",
      "We were given the constraint that the functional margin is:\n",
      "\n",
      "* $y^i (w^T x^i + b) \\geq 1 $\n",
      "\n",
      "We re-wrote this constraint to fit the primary problem:\n",
      "\n",
      "* $g_i(w) = -y^i (w^T x^i + b) + 1 $    \n",
      "* $g_i(w) \\leq 0$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use this to maximize the geometric margin for a new problem, written in primal form:\n",
      "\n",
      "* $min_{w}$ $f(w) = \\frac{1}{2} w^T w $.\n",
      "* Subject to: $w^T x^i \\geq 1$.\n",
      "\n",
      "For this new problem, we should similarly re-write the constraint:\n",
      "\n",
      "* $g_i(w) = 1-(w^T x^i)$\n",
      "* $g_i(w) \\leq 0$\n",
      "\n",
      "And we write the Lagrangian:\n",
      "\n",
      "$$ L(w,b,\\alpha) = \\frac{1}{2} w^T w + \\sum\\limits_{i = 1}^{k} \\alpha_i [1-(w^T x^i)] $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the dual form of the problem, we first minimize the Lagrangian with respect to $w,b$.\n",
      "\n",
      "Since there is no $b$ parameter, we simply take the gradient of the Lagrangian with respect to $w$:\n",
      "\n",
      "$$ \\nabla_w L(w,b,\\alpha) = w + \\sum\\limits_{i=1}^k - a_i x_i $$\n",
      "\n",
      "We use this to find our optimal value of $w$:\n",
      "\n",
      "$$ w = \\sum\\limits_{i=1}^k a_i x_i $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that our initial framing of the dual problem:\n",
      "\n",
      "$$ D^* = max_{\\alpha,\\beta,\\alpha \\geq 0} min_{x} L(w,\\alpha,\\beta) $$\n",
      "\n",
      "As shown above, we have done:\n",
      "\n",
      "$min_{x} L(w,\\alpha,\\beta)$\n",
      "\n",
      "This has given us the optimal value of $w$.\n",
      "\n",
      "We plug it back into the Lagrangian:\n",
      "\n",
      "$$ D^* = max_{\\alpha \\geq 0} L(w,b,\\alpha) = \\frac{1}{2} ( \\sum\\limits_{i=1}^k a_i x_i)^T ( \\sum\\limits_{i=1}^k a_i x_i) + \\sum\\limits_{i = 1}^{k} \\alpha_i [1-(( \\sum\\limits_{i=1}^k a_i x_i)^T x^i)] $$\n",
      "\n",
      "Distribute terms and simplify:\n",
      "\n",
      "$$ D^* = max_{\\alpha \\geq 0} L(w,b,\\alpha) = \\sum\\limits_{i = 1}^{k} \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^k \\sum\\limits_{j=1}^k a_i a_j (x_i)^T x^j $$\n",
      "\n",
      "Note that this can be Kernalized, because only inner products of the data appear!\n",
      "\n",
      "To evaluate a point, $z$, we simply evaluate $w^Tz$ with $w^T = (\\sum\\limits_{i=1}^k a_i x_i)^T $."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Inner products -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a distance between vectors (inner product). It is essential to have a positive inner product.\n",
      "\n",
      "Furthmore, you can think of the inner product as a measure of distance.\n",
      "\n",
      "* If two vectors are orthogonal, their inner product will be small.\n",
      "* Conversely, similar vectors will have a large inner product.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The inner product can be represented as a Kernel function - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have previously been discussing linear models, which lead to a linear decision boundary:\n",
      "\n",
      "As with logistic regression, the decision boundary is produced when $\\omega^Tx + b = \\omega_1 x_1 + \\omega_2 x_2 + b = 0$.\n",
      "\n",
      "The linear decision boundary is:\n",
      "\n",
      "$x_2 = -\\frac{\\omega_1}{\\omega_2} x_1 - \\frac{b}{\\omega_2} $\n",
      "\n",
      "However, it may be useful to learn in a higher dimentional space (e.g., with respect to $x^2$, etc).\n",
      "\n",
      "For this, the idea of a feature mapping, $\\phi$, is a way to re-cast the attributes in a higher dimentional space.\n",
      "\n",
      "$$ \\phi(x) = \\begin{bmatrix} x_i x_j \\\\   \\\\   \\end{bmatrix} $$\n",
      "\n",
      "If we want to learn in this higher dimensional space using our new funtion (above), we need to replace $x$ with $\\phi(x)$.\n",
      "\n",
      "In turn, we need to take the dot product of features, $\\phi(x)$.\n",
      "\n",
      "$$ \\phi(x)^T \\phi(z) = \\sum\\limits_i \\sum\\limits_j x_i x_j z_i z_j = \\sum\\limits_i x_i z_i \\sum\\limits_j x_j z_j = (x^T z)^2 $$\n",
      "\n",
      "In turn, this is the Kernel given the mapping specified above. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Work a more complex example to further highlight the principle:\n",
      "\n",
      "$$ \\phi(x) = \\begin{bmatrix} x_i x_j \\\\ ...  \\\\ x_3 x_3 \\\\ \\sqrt{2c}x_i \\\\ ... \\\\ \\sqrt{2c}x_3 \\\\ c \\end{bmatrix} $$\n",
      "\n",
      "$$ \\phi(x)^T \\phi(z) = \\sum\\limits_i x_i z_i \\sum\\limits_j x_j z_j +  \\sum\\limits_i \\sqrt{2c}x_i \\sqrt{2c}z_i + c^2 = (x^T z)^2 + \\sqrt{2c} (x^T z) + c^2 $$\n",
      "\n",
      "Note:\n",
      "$$ A^2 + 2AB + B^2 = (A+B)^2 $$\n",
      "\n",
      "Thus:\n",
      "\n",
      "$$ K(\\phi(x),\\phi(z)) = \\phi(x)^T \\phi(z) = (x^T z + C)^2 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are more interesting mapping functions, which may lead to a Kernel definition.\n",
      "\n",
      "Intuitively, the Kernel mapping shows that the inner product in a higher dimentional space is equal:\n",
      "\n",
      "* To the inner product computer in the lower dimentional space with $x,z$.\n",
      "* This reduces the number of computations considerably.\n",
      "* Moreover, this is a measure of the similarly of two attribute vectors mapped by $\\phi$ into a higher dimentional space.\n",
      "\n",
      "So, a Kernel funtion, $K$, is intuitively a measure of distance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Prooving valid Kernels - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The challenge: how can we tell whether some kernel function is valid? \n",
      "\n",
      "* Is there some feature mapping $\\phi$ so that $ K(x,z) = \\phi(x)^T \\phi(z) $?\n",
      "\n",
      "Now, how can we tell that K is a valid kernel for a given feature mapping $phi$?\n",
      "\n",
      "First, we define the idea of the Kernel matrix. \n",
      "\n",
      "Over a set of $m$ points, the Kernel matrix is given by:\n",
      "\n",
      "$$K_{ij} = K(x^i,x^j)$$\n",
      "\n",
      "If K is a valid kernel:\n",
      "\n",
      "* $K_{ij} = K(x^i,x^j) = \\phi(x^i)^T \\phi(x^j) ... = K_{ji}$. Thus, the matrix must be symmetric.\n",
      "* K is positive semi-definite: $z^T K z = \\sum\\sum z_i K_{ij} z_j \\geq 0$\n",
      "\n",
      "Operations applied to kernels, assuming $K_1$ and $K_2$ are valid kernels:\n",
      "\n",
      "* Summation: Kernel. $ K_3(x,z) = K_1(x,z) + K_2(x,z) \\geq 0 $\n",
      "* Subtraction: Not a kernel (e.g., if $K_2(x,z) = 2K_1(x,z)$, then $ K_1(x,z) - K_2(x,z)$ is not $\\geq 0$. \n",
      "* Multiplication by a positive real number: Kernel.\n",
      "* Powers: Long proof (1e). Kernel.\n",
      "* Constant term: Kernel. But why?\n",
      "* Polynomial: Kernel.\n",
      "\n",
      "These can be used to derive new Kernel functions. \n",
      "\n",
      "(1) The exponential function:\n",
      "$$ K_i(x,z) = exp(K(x,z)) = \\sum\\limits_{j=1}^i K(x,z)^j $$\n",
      "\n",
      "* This uses the Taylor expansion of $e^x$.\n",
      "* This is a polynomial and we prooved that polynomials are valid Kernels. \n",
      "* So, $K_i(x,z)$ is a valid kernel.\n",
      "\n",
      "(2) The Gaussian kernel:\n",
      "$$ K(x,z) = exp(-\\| x-z \\|^2 / \\sigma^2) $$\n",
      "\n",
      "* This uses the trick: $ \\| x-z \\| = \\| x \\|^2 - 2x^Tz + \\| z \\|^2 $\n",
      "\n",
      "$$ K(x,z) = [ exp(-\\| x \\|^2 / \\sigma^2) exp(-\\| z \\|^2 / \\sigma^2) ] exp(2x^Tz / \\sigma^2) $$\n",
      "\n",
      "* The first two terms are a valid kernel from \"Constant term\" above?\n",
      "* The final term is $e^{K(x,z)}$, which is a valid kernel?\n",
      "* The product of two kernels is valid.\n",
      "* So, this is a valid kernel.\n",
      "\n",
      "To-do:\n",
      "\n",
      "* Run additional Kernel problem from midterm.\n",
      " "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Utility of Kernels - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If an algorithm inclues inner products $<x, z>$ between attribute vectors, then we can replace this with K(x,z) where K is a kernel. This allows the algorithm to work efficiently in the high dimensional feature space corresponding to K at low computational cost.\n",
      "\n",
      "Example:\n",
      "\n",
      "* Seperate two classes that are represented in a 2D space as concentric circles.\n",
      "* Define $\\phi$ as a mapping that will transform x,y into a (higher) dimentional space in which the classes are seperated."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ability to evaluate performance after mapping - "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Non linearly seperable data - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the optimal margin classifier worked well for linearly seperable data:\n",
      "\n",
      "However, the constraint that all training examples must have a functional margin $> 1$ does not work well for outliers.\n",
      "\n",
      "Moreover, we can re-formulate our problem from the optimal margin classifier to a $\\gamma$ term:\n",
      "\n",
      "* $f(w) = \\|w\\|^{2} + C \\sum_{i=1}^m \\gamma_i $\n",
      "* $ y^i (w^T x^i + b) \\geq 1 - \\gamma_i $\n",
      "* $ \\gamma_i \\geq 0$ \n",
      "\n",
      "Thus, the objective gets increaced by $C \\gamma_i$ for any training examples with function margin $1-\\gamma_i$!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Sequential minimal optimization (SMO) - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall previously:\n",
      "\n",
      "* We set up the dual optimization problem to maximize the margin of the optimal margin classifier.\n",
      "* We found the optimal value of $w$ setting the gradient of $L(w,b,\\alpha)$ with respect to $w$ equal to zero.\n",
      "* We found an additional equality constraint by finding the derivative of $L(w,b,\\alpha)$ with respect to $b$ equal to zero.\n",
      "* We plugged the optimal value of $w$ back into the Lagrangian to solve the second part of the problem $max_{\\alpha \\geq 0} L(w,b,\\alpha)$ and get $\\alpha_i$.\n",
      "* We kept the equality constraint $\\sum_{i=1}^m \\alpha_i y^i = 0$.\n",
      "\n",
      "However, we did not discuss how to actuall solve $max_{\\alpha \\geq 0} L(w,b,\\alpha)$.\n",
      "\n",
      "So, recall the final form of the dual problem:\n",
      "\n",
      "$$ max_{\\alpha; \\alpha \\geq 0} W(\\alpha) = \\sum \\alpha_i - 0.5 \\sum_i \\sum_j y^i y^j \\alpha_i \\alpha_j - <x^i , x^j > - b \\sum \\alpha_i y^i $$\n",
      "\n",
      "With constraints and training data labels $y^i$:\n",
      "\n",
      "* $0 \\leq \\alpha_i \\leq C$\n",
      "* $\\sum_{i=1}^m \\alpha_i y^i = 0 $\n",
      "\n",
      "Critically, we cannot independently vary a single parameter, $\\alpha_i$, due to the second constraintL\n",
      "\n",
      "* $\\sum_{i=1}^m \\alpha_i y^i = 0 $:\n",
      "\n",
      "So, the algorithm will simply:\n",
      "\n",
      "* Pick a pair of $\\alpha_{i,j}$\n",
      "* Re-optimize $W(\\alpha)$ with respect to $\\alpha_{i,j}$ with the rest fixed.\n",
      "\n",
      "Thus, we have:\n",
      "\n",
      "$$ \\alpha_1 y^1 \\alpha_2 y^2 = \\sum_{i=3}^m \\alpha_i y^i $$\n",
      "$$ \\alpha_1 y^1 \\alpha_2 y^2 = \\Gamma $$\n",
      "\n",
      "The key point is that we can re-write: \n",
      "\n",
      "* $ \\alpha_1 = g(\\alpha_2) $\n",
      "* $ W(g(\\alpha_2), \\alpha_2, ...) $\n",
      "\n",
      "Thus, $W()$ is a quadratic function of $\\alpha_2$, which can be minimized. \n",
      "\n",
      "This minimization can be applied iteratively across the parameters!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Sequential minimal optimization (SMO) - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Application of SMO to a different minimization problem, with the following objective:\n",
      "\n",
      "$$ D^* = max_{\\alpha \\geq 0} L(w,b,\\alpha) = \\sum\\limits_{i = 1}^{k} \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^k \\sum\\limits_{j=1}^k a_i a_j (x_i)^T x^j $$\n",
      "\n",
      "Critically, in this case there was no $b$ parameter in the Lagrangian:\n",
      "\n",
      "$$ L(w,b,\\alpha) = \\frac{1}{2} w^T w + \\sum\\limits_{i = 1}^{k} \\alpha_i [1-(w^T x^i)] $$\n",
      "\n",
      "In turn, we never had to compute the derivative of $L(w,b,\\alpha$) with respect to $b$.\n",
      "\n",
      "Thus, do not have the equality constraint that previously forced us to iterate over two parameters, $\\alpha_{i,j}$, at the same time: $\\sum_{i=1}^m \\alpha_i y^i = 0 $:\n",
      "\n",
      "In turn, we can iterate over a single parameter, $\\alpha_i$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Moreover, we simply set the derivative of $D^*$ equal to zero.\n",
      "\n",
      "Recall:\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial \\alpha_i} \\sum\\limits_{i=1}^k \\sum\\limits_{j=1}^k a_i a_j $$\n",
      "\n",
      "In the outer sum, all terms except $\\alpha_i$ drop.\n",
      "\n",
      "We then multiply $\\alpha_i$ with all elements in $\\alpha$, with $2$ indicating that we are not concerned with order:\n",
      "\n",
      "$$  \\frac{\\partial}{\\partial \\alpha_i} \\alpha_i^2 + 2 \\sum_{i \\neq j} a_i a_j = 2 \\alpha_i + 2 \\sum_{i \\neq j} \\alpha_j $$\n",
      "\n",
      "Therefore, using $K(x_i,x_j)$:\n",
      "\n",
      "$$ 0 = D^* = 1 - (\\alpha_i + \\sum_{i \\neq j} \\alpha_j) K(x_i,x_j) $$\n",
      "\n",
      "Solve for $\\alpha_i$!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learning Theory"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Key concepts - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Bias / variance tradeoff\n",
      "* Hoeffding inequality\n",
      "* Uniform convergence\n",
      "* VC dimension"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Introduction - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Generalization error}$ occours when our model does not generalize well to data outside the training set. This has two features:\n",
      "\n",
      "* Bias: Generalization error even if we were to fit it to a very (say, infinitely) large training set. A linear model fit to non-linear data (e.g., underfit) has high bias.\n",
      "* Variance: Generalization error if we fit \u201cspurious\u201d patterns in the training set. A high order polynomail fit to the noise in linear data (e.g., overfit) has high variance.\n",
      "\n",
      "The trade-off between these can be observed: a complex model with many parameters may have low Bias, but high Variance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two Lemmas are useful for learning theory:\n",
      "\n",
      "The union bound: \n",
      "\n",
      "* $P(A_1 \\cup ... \\cup A_k) \\leq P(A_1) + ... + P(A_k)$\n",
      "* Probability of any one of k events happening is at most the sums of the probabilities of the k different events.\n",
      "\n",
      "\n",
      "Hoeffding inequality intuitivly says the following:\n",
      "\n",
      "* The probability of heads is $\\phi$\n",
      "* If the coin is tossed $m$ times, the fraction of times it comes up heads is a good estimate of $\\phi$ if $m$ is large.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a training set:\n",
      "    \n",
      "* $\\textbf{Training error}$, $\\hat{\\epsilon}$ is the fraction of samples that our hypothesis misclassifies.\n",
      "* $\\textbf{Generalization error}$ is probability that a new sample will be misclassified.\n",
      "\n",
      "One way to fit the parameters, $\\theta$, is to minimize the training error, resulting in $\\hat{\\theta}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "VC Dimension -"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\textbf{Hypothesis space}$ is the set of hypotheses.\n",
      "\n",
      "* For example, we can consider the set of all linear decision boundary classifiers over the domain of inputs, $X$.\n",
      "* Empirical risk minimization simply selects the function, $h$, with the smallest training error (it minimizes  $\\hat{\\epsilon}$).\n",
      "\n",
      "$\\textbf{VC dimension}$ of a hypothesis set H is the most points, $s$, (the largest set) that H can shatter.\n",
      "\n",
      "* Where shatter is defined as zero training error (or, perfect classification).\n",
      "* To proove $VC(h)$ is at least $s$, we simply must show that there's at least $one$ set of size $s$ that.\n",
      "* The number of training examples needed to learn well is linear in the VC dimention of $h$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Useful math - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "--- $\\textbf{Useful rules for exponents and logs}$ ---\n",
      "\n",
      "$$ log(xyz) = logx + logy + logz $$\n",
      "\n",
      "$$ log(x) - log(y) = log(x / y) $$\n",
      "\n",
      "$$ log(x^y)=y log(x) $$\n",
      "\n",
      "$$ x^y = exp(log(x^y)) = exp(y log x) $$\n",
      "\n",
      "$$ x^m + x^n = x^{m+n} $$\n",
      "\n",
      "$$ (z^T z) = z^2 $$\n",
      "\n",
      "$$ (a^2)((b^2) = (ab)^2 $$\n",
      "\n",
      "$$ log (c e^x) = x ? $$ \n",
      "\n",
      "--- $\\textbf{Useful rules for derivatives}$ ---\n",
      "\n",
      "$$ \\frac{d}{dx} e^x = e^x \\frac{d}{dx}(x)  $$\n",
      "\n",
      "$$ \\frac{d}{dx} log(f(x)) = \\frac{f'(x)}{f(x)} $$\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial \\theta_k} (\\sum\\limits_{i=1}^m \\theta_i (x_i)) = x_k  $$\n",
      "\n",
      "If $h_{\\theta} (x)$ is a sigmoid function:\n",
      "\n",
      "$$ \\frac{\\partial h_{\\theta} (x) } { \\partial \\theta_k } = h_{\\theta} (x ) (1 - h_{\\theta} (x)) $$\n",
      "\n",
      "Given, $a(\\eta)$ where $\\eta = \\theta^T (x) $ apply the chain rule:\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial \\eta} a(\\eta) = \\frac{\\partial}{\\partial \\eta} a(\\eta) \\times  \\frac{\\partial}{\\partial \\theta_i} \\eta $$\n",
      "\n",
      "--- $\\textbf{Useful linear algebra}$ ---\n",
      "\n",
      "$$ zHz^T = \\sum\\limits_{i=1}^m \\sum\\limits_{i=1}^m z_i z_j h_{ij} $$\n",
      "$$ z^Tz = \\sum\\limits_{i}z_i^2 $$\n",
      "Trace is the sum of diagonal elements in a matrix.\n",
      "$$ trA = \\sum_{i=1}^n A_{ii} $$\n",
      "$$trAB = trBA $$\n",
      "$$ trA = trA^T$$\n",
      "\n",
      "The gradient, $\\nabla f(A)$, is a m-by-n matrix (in dimention of A) with each element specified by \\frac{\\partial f}{\\partial \\A_{ij}}.\n",
      "\n",
      "Vectorize quantities:\n",
      "\n",
      "If $H_{ij} = z_i z_j$ then $H = z^T z$.\n",
      "\n",
      "\n",
      "--- $\\textbf{Optimization}$ ---\n",
      " \n",
      "$$ argmin_{\\theta} f(x) = argmax_{\\theta} -f(x) $$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Study to-do - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Review convex optimization (Wed, 11/5/14) - \n",
      "\n",
      "* 141024-cs229-720.mp4\n",
      "* 141017-cs229-720.mp4\n",
      "\n",
      "SVM problems (Wed, 11/5/14) - \n",
      "\n",
      "* SVM long-form and short-answer problems from midterm.\n",
      "\n",
      "Learning theory (Thurs, 11/6/14) - \n",
      "\n",
      "* Properties of VC dimension problem from PS#2.\n",
      "* Short-answer problem from midterm.\n",
      "\n",
      "Newton's method (Thurs, 11/6/14) - \n",
      "\n",
      "* Run Newton's method problem from PS#1.\n",
      "* Run midterm problem on Newton's method.\n",
      "\n",
      "Perceptron (Thurs, 11/6/14) - \n",
      "\n",
      "* Review problem on Kernelizing the perceptron from PS#2.\n",
      "* Review midterm problem.\n",
      "\n",
      "GDA (Fri, 11/7/14) -\n",
      "\n",
      "* Re-work problem from P Set #1.\n",
      "* Work problem from Midterm review.\n",
      "* Connection to logistic regression.\n",
      "\n",
      "Softmax regression (Fri, 11/7/14) - \n",
      "\n",
      "* Define.\n",
      "\n",
      "Bayes rule and Naive Bayes (Fri, 11/7/14) -\n",
      "\n",
      "* Re-work problem.\n",
      "\n",
      "Uniform Convergence (Fri, 11/7/14) - \n",
      "\n",
      "* Mid-term problem.\n",
      "\n",
      "Mutual information (Fri, 11/7/14) - \n",
      "\n",
      "* Mid-term problem.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}